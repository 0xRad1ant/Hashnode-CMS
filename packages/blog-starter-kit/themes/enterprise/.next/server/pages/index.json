{"pageProps":{"allPosts":[{"id":"64f724e54ae4dfa2e3d4d440","title":"The art of feed curating: Our approach to generating personalized feeds that match users' interests","url":"https://engineering.hashnode.com/the-art-of-feed-curating-our-approach-to-generating-personalized-feeds-that-match-users-interests","content":{"markdown":"Feeds are an essential part of every social network. The same applies here at Hashnode. Until now, we have used a very basic and generic algorithm to generate feeds mainly based on [hot ranking algorithms.](https://saturncloud.io/blog/how-are-reddit-and-hacker-news-ranking-algorithms-used/#how-hot-ranking-works) Over time, we noticed that users struggle to find the content they are genuinely interested in on our platform. This is why we have decided to power up our feed game with personalized feeds üéâ\n\n### Why are personalized feeds such a big deal? ü§î\n\nWhen you visit a community platform, you want to see content that speaks to you. That's where personalized feeds come in! Instead of a generic feed, personalized feeds focus on what the user finds interesting and valuable. They do this by looking at your past interactions, interests, and other factors to serve up content that's right up your alley üéØ\n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">üí°</div>\n<div data-node-type=\"callout-text\">Personalized feeds at Hashnode focus on what YOU find interesting!</div>\n</div>\n\nAs a result, users get an enjoyable and engaging experience. Personalized feeds help to keep users happy and active by catering to their preferences and interests.\n\nLet's explore how we implemented personalized feeds at Hashnode üöÄ\n\n## Personalized feeds for different users ‚Äì let‚Äôs see the result\n\nLet's examine the concept of personalization. From a user's standpoint, assuming that the feed will differ for each individual is reasonable. When we query our feed endpoint for two distinct users, we obtain the following results:\n\n![Comparing Feeds - Result for a User](https://cdn.hashnode.com/res/hashnode/image/upload/v1693294681441/869135e2-d7db-4ed0-b81c-faaa6fb54f22.gif align=\"center\")\n\n---\n\n![Comparing Feeds - Result for a User](https://cdn.hashnode.com/res/hashnode/image/upload/v1693294707867/3593c39f-2463-4067-98d0-79a61dfeb390.gif align=\"center\")\n\nThe users follow different tags and engage with Hashnode in unique ways. The outcome? A feed that is tailored to each user.  \nIf you want to see this in action, visit [our Homepage](https://hashnode.com/).\n\nLet's see how we have built this experience from an engineering perspective üëÄ\n\n## How to generate personalized feeds without machine learning?\n\nFor most platforms, machine learning is the go-to approach to calculate and personalize stuff. These techniques and algorithms have existed for a long time and are rooted in the early e-commerce systems to recommend other stuff that you may find interesting.  \nPlatforms like Facebook, Twitter, and Instagram have shown machine learning models can be effective when trained with enough and correct data.\n\n![a robot sitting on top of a wooden bench](https://images.unsplash.com/photo-1620712943543-bcc4688e7485?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=1000&q=80 align=\"left\")\n\nAt Hashnode, we decided initially not to go the machine learning route for our personalized feeds. Although incorporating machine learning into our platform is a long-term objective, we have opted for an alternative approach in the interim.\n\nStarting with an ML approach is challenging. You need to have some knowledge about creating machine learning models and pipelines. You need to integrate ML into your existing platform. Multiple options are available: either go with self-hosting and self-creating/training your models or use a service where you can train models by feeding them data. The one point that stuck out was that you must understand your data and what you want to achieve with the model.\n\nWe decided to go with a ranking-based approach, as we wanted to verify assumptions we already had about our content and get out a better algorithm as fast as possible. To generate personalized feeds without machine learning, we have developed our own unique recipe for feed generation. This method considers various aspects and user behavior patterns that we believe will be most beneficial in ranking posts for each user. By carefully considering multiple factors, we can curate a feed that caters to our users' specific interests and needs.\n\n![a diagram of algorithm with different inputs to generate personalized feeds](https://cdn.hashnode.com/res/hashnode/image/upload/v1692887045099/9eca6989-c052-43bb-846b-a64b06fabbed.png align=\"center\")\n\nGoing with this route gives us the leverage to understand the influence different weights and data points have on the quality of our feed. It enables us to provide a more customized experience for our community members and continually refine and improve our feed generation process. Resulting in a faster delivery of even more relevant and captivating content as we gather more data and insights on user preferences and behavior.\n\nIn conclusion, while machine learning and AI algorithms are powerful tools for generating personalized feeds, traditional techniques can still be employed to create a tailored content experience.\n\nLet's look at what we use to develop a personalized feed for a user ü´£\n\n## Which data is relevant for feed calculation?\n\n![different inputs that are considered while calculating a personalized feed for a user ](https://cdn.hashnode.com/res/hashnode/image/upload/v1692886298254/fe479a1d-0835-458b-b399-14b1cb9acb65.png align=\"center\")\n\nTo effectively personalize feeds and generate accurate content rankings for each user, it is crucial to consider several pieces of data that can provide valuable insights into their preferences and behavior. These data points include:\n\n**User-specific:**\n\n* Followed Tags: The tags a user follows indicate their topics of interest. Content with matching tags should be prioritized higher.\n    \n* Following the Author/Blog: A user who follows an author or blog will likely enjoy that content source. Content from followed authors/blogs should be weighted more heavily.\n    \n* Reading History: What articles a user has read in the past shows their preferred content types and subjects. Similar new content should be ranked higher.\n    \n\n**Community-specific**:\n\n* Likes: If an article has received many likes from the community, it is likely high quality and relevant to some users.\n    \n* Comments: More engagement in comments also indicates relevance and popularity.\n    \n* Views: Higher-viewed content is generally more relevant to more users.\n    \n* Featured: Featured articles are chosen for their relevance and quality.\n    \n* Recency: Newer content is likely fresher and more timely. Furthermore, this boosts newly published content and freshens up the feed.\n    \n\n**Blog-specific**:\n\n* Pro Account: Pro on a blog can indicate that the authors use Hashnode on a more sophisticated level and leverage features such as Hashnode AI, which allows them to generate even more high-quality content.\n    \n* Custom Domain: Custom domains indicate authors leveraging Hashnode to build their brand and publish high-quality content.\n    \n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">üí°</div>\n<div data-node-type=\"callout-text\">The above list shows roughly the importance of the weights in a declining order. The main focus lies on the User-specific weights, whereas the following categories have less and less influence on the score.</div>\n</div>\n\n## Ranking posts to generate personalized feeds\n\nNow, it's time to look at how weights are calculated and how they influence the score of a post within a user's feed. üßë‚Äçüíª\n\nThe User-specific weights are rather straightforward. We can check if the user is following the Author/Blog, the following tags added to the post, and if the article is from an author in the reading history of the user:\n\n```typescript\n  const followingScore = usersUserIsFollowing.includes(\n    post.author\n  )\n   ? FOLLOW_WEIGHT\n   : 0;\n\n  const readingHistoryScore = postsAuthorsUserHasViewed.includes(\n     post.author\n   )\n    ? READING_HISTORY_WEIGHT\n    : 0;\n\n   const tagScore = calculateTagWeights(\n     tagsUserIsFollowing,\n     post.tags\n   );\n\n   const userSpecificScore = followingScore \n                              + readingHistoryScore\n                              + tagScore;\n```\n\nFor the Community-specific weights, this is a little bit more difficult. We could go with a straightforward approach by deciding a weight for a single like and multiplying this with the likes this post has received, but the chance to over-boost a post becomes very high when going this route. Furthermore, this would not only result in potential over-boosts and decrease the relevance for the user, but it would also result in a non-uniform distribution of likes. Let's take a look at an example. Assuming two posts and a weight of 2 for each, like:\n\n$$10(likes) * 2 = 20$$\n\nand\n\n$$100(likes) * 2 = 200$$\n\n‚û°Ô∏è There is no way to compare these two as the values are too far apart.\n\nThe solution we came up with is twofold:\n\n* Normalize likes, views, and comments\n    \n* Treat the weight for these parameters as the maximum a post can receive. For this, we need to update how we calculate the weight.\n    \n\nThe formula can then look relatively easy. Let's retake a look at the likes for a post:\n\n```typescript\n// Baseline value, everything over 1000 likes will recive the\n// full score for the likes weight. Everything below will be fracitonal\n// For 1000 as basline this will be 3\nconst MAX_LIKE = Math.log10(1000);\n\n// For 10 likes this will be 1\n// For 100 likes this will be 2\nconst postLikes = Math.log10(post.likes)\n\n// Assuming 10 likes will result in: 1 * 2 / 3 = 0.66\n// Assuming 100 likes will result in: 2 * 2 /3 = 1.33\nconst scoreForLikes = (post.likes * LIKES_WEIGHT) / MAX_LIKE;\n\n// Check if the score is greater than the LIKES_WEIGHT\n// Yes -> use LIKES_WEIGHT\n// No -> use the calculated score \nconst actualLikeScore = scoreForLikes > LIKES_WEIGHT ? LIKES_WEIGHT : scoreForLikes\n```\n\nAs we can see in the above snippet, this way of calculating the scores will ensure that:\n\n* An article with many likes is not over-boosted\n    \n* The score does not exceed the weight we set as a maximum\n    \n* The values are more evenly distributed, comparing `20` to `200` (`10` times) vs. `0.66` to `1.33` (roughly `2` times)\n    \n\nThis is done for all the Community-specific weights except the featured flag, which can be easily added by checking if the post is featured on Hashnode.\n\nOn the other hand, Recency also needs a specific logic to give us a desired score so we do not overboost newly published articles.\n\n```typescript\nconst getDateFactorForFeed = (date: Date) => {\n  // Consider the last 30 days in hours\n  const recentTimeFrame = 720;\n\n// Divide the recent weight by the time frime to get the points \n// each hour will recive\n// e.g. 5 / 720 = 0.0069444444\n  const pointsPerHour = RECENT_WEIGHT / recentTimeFrame;\n\n// Calculate the difference in hours between now and the publish date\n// of the article \n  const difference = dayjs().diff(dayjs(date), 'hours');\n\n// The weight should not be negative. \n  const weight = Math.max(recentTimeFrame - difference, 0);\n\n// Multiple the resulting weight with the points for each hour\n// weight = 0 => 0 * 0.0069444444 = 0\n// weight = 720 => 720 * 0.0069444444 = 5\n  return weight * pointsPerHour;\n};\n```\n\nWith this calculation, the maximum value recency can receive will always be `RECENT_WEIGHT`.\n\nLastly, the Write-specific weights are calculated similarly to User-specific weights by checking if the Blog has connected a custom domain or is subscribed to Hashnode Pro.\n\nThe overall score calculation for the specific post adds up all our values, and we have a score for the post üöÄ\n\n## How we keep your feed fresh all the time\n\nAfter the initial testing, we noticed that the feed displays more relevant content but does not refresh as frequently as we would like.\n\n**How to make the feed highly dynamic and present fresh content on every visit?ü§î**\n\nThere are multiple possible solutions to do this, but we decided to explore damping. We don't want to harshly penalize articles by removing them from feeds. With damping, posts receive a slight reduction in their score if they have already been presented to the user. Moreover, this allows them to reappear in the feed and be noticed by the user. To ensure fair treatment for all articles, we have decided on the following rules:\n\n* The damping is based on the page an article is presented on\n    \n* As the number of pages increases, the damping of articles on those pages decreases.\n    \n* We will only implement damping for the first five pages, ensuring we do not inadvertently exclude any articles beyond that point.\n    \n\nAfter 24 hours, we remove all damping effects from a user's feed, allowing each post to potentially resurface in the upper positions.\n\nIn combination with the score calculation algorithm, this has a very nice effect:\n\n\n<div style={{ maxWidth: \"100%\", margin: \"0 auto\" }}>\n  <iframe\n    src=\"https://veed.io/embed/530d4a96-da87-4436-9a34-9cf2ec1b0efc\"\n    width=\"100%\"\n    height=\"504\"\n    style={{ border: \"none\" }}\n    allowFullScreen\n  />\n</div>\n\n## Next plans for the personalized feeds on hashnode\n\nNow that we have created an algorithm and prepared the groundwork, what comes next for our new personalized feed?\n\nI assume you guessed it right:\n\n%[https://media.giphy.com/media/sGQczHZ49ICQTTXacn/giphy.gif] \n\nInitially, we only wanted to verify our assumptions about content and how to create a feed that engages users and shows high-quality and highly relevant articles from our Platform.\n\nAs you can imagine, the calculation is expensive from a computational point of view. We need to gather the latest posts from our platform, collect data for the user, and calculate the scores for each post before we can serve them. It is also not as straightforward as it would be with a following feed. There, we could cache everything and append new posts on the top of the cache to be served on a request to the feed.\n\nIn the case of a personalized feed, we need to have the user metadata at hand to correctly assign a score to the article for the user based on the algorithm.\n\nGuess what? We've developed a nice solution that I'll dive into in another article! But here's a little sneak peek: We're pre-calculating the personalized feed for all our active users on the platform! This way, we can slash peak loads on our service and serve the feed at lightning-fast speeds! üöÄüí•üèéÔ∏è\n\nUntil the next one,\n\nCheers üôå"},"author":{"name":"Florian Fuchs","profilePicture":"https://cdn.hashnode.com/res/hashnode/image/upload/v1675085454170/766f4dd9-9fba-4aa7-b141-ff62bba7eafe.jpeg"},"coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/upload/v1693400608045/24b21141-0ceb-4827-a510-1f3dfee5e41f.jpeg"},"publishedAt":"2023-09-05T12:53:57.776Z","slug":"the-art-of-feed-curating-our-approach-to-generating-personalized-feeds-that-match-users-interests","brief":"Feeds are an essential part of every social network. The same applies here at Hashnode. Until now, we have used a very basic and generic algorithm to generate feeds mainly based on hot ranking algorithms. Over time, we noticed that users struggle to ..."},{"id":"64d492016f3feecb8702c3b4","title":"CI Checks: Ensuring Better Code Quality and Faster Deployment","url":"https://engineering.hashnode.com/ci-checks-ensuring-better-code-quality-and-faster-deployment","content":{"markdown":"How can you consistently deliver high-quality code that adheres to established coding guidelines and is free from errors?  \nThe solution lies in implementing tests and multiple checks for linting and type errors.\n\nThis may seem straightforward, but it requires some adjustments to smooth out the developer experience (DX) flow and maintain developer productivity.\n\nIn this article, we'll explore how Hashnode previously managed its development workflow and the improvements it made to ensure better code quality and faster deployment through CI checks.\n\n## Goal\n\nThe goal is to establish a rapid feedback loop for developers, allowing them the freedom to experiment and move fast without being burdened by coding guidelines and the like.\n\nOf course, we still aim to enforce coding guidelines and ensure that whatever reaches production is error-free and passes all checks. However, we want to accomplish this without hindering developer productivity and utilize tools at our disposal.\n\n## Previous Workflow\n\nThe previous workflow involved running each check on the developer's machine before they commit using pre-commit hooks. This involved enforcing coding guidelines, format commit messages and a bunch of other checks.\n\nThese guidelines were necessary but enforcing them at the pre-commit level was not a great idea. Developers should be allowed to code how they want and this was hurting the productivity.  \nLet's take a look at the different checks we had at the commit level.\n\n### Type Checks\n\nWe use TypeScript heavily to ensure the data types used in a codebase align with the expected types and catch potential errors or inconsistencies early in the development process. Usually, the IDE takes care of complaining whenever a certain function or component doesn't satisfy the types it was supposed to.\n\nHowever, it is easy to overlook these warnings for pages that are out of the scope of the feature in development.\n\nFor this reason, it is necessary to perform type checks for every commit to guarantee that no issues arise throughout the codebase. This can be accomplished by using:\n\n```bash\ntsc --noEmit\n```\n\nThis simple command will execute a type check on the entire codebase and generate an error if any issues are detected.\n\n### Linting\n\nWe use ESLint to enforce coding guidelines, ensuring that everything is in order. It also enforces aspects such as import order and accessibility checklists. To run lint checks on the relevant files, use the following command:\n\n```bash\neslint . --fix --ext .js,.jsx,.ts,.tsx\n```\n\nThese are plugins that we use at Hashnode\n\n```json\n{\n    \"eslint\": \"^7.24.0\",\n    \"eslint-config-airbnb\": \"^18.2.1\",\n    \"eslint-config-airbnb-base\": \"^14.2.1\",\n    \"eslint-config-next\": \"13.0.5\",\n    \"eslint-config-prettier\": \"^8.3.0\",\n    \"eslint-import-resolver-typescript\": \"^2.4.0\",\n    \"eslint-plugin-cypress\": \"^2.11.3\",\n    \"eslint-plugin-import\": \"^2.22.1\",\n    \"eslint-plugin-jsx-a11y\": \"^6.4.1\",\n    \"eslint-plugin-prettier\": \"^3.4.0\",\n    \"eslint-plugin-react\": \"^7.23.2\",\n    \"eslint-plugin-react-hooks\": \"^4.2.0\",\n    \"@typescript-eslint/eslint-plugin\": \"^5.39.0\",\n}\n```\n\n### Tests\n\nWe use React Testing Library to write tests for all critical business flows. These tests run on GitHub Actions and report any issues if something is broken. We also have merge rules in place for pull requests, ensuring that only tested code is allowed to merge and proceed to production.\n\n### Husky\n\n[Husky](https://typicode.github.io/husky/) is a pre-commit hook that ties everything together. We configured Husky to perform type checks and lint checks on every commit. This ensures that we only commit error-free code and block pushes if something is broken. Since tests take some time to run, we decided to keep tests at the CI level and not run them locally for every commit.\n\n## The Problem\n\nAll of this is excellent and has been working effectively; however, there was a problem. As the codebase began to expand, running all these checks took between 3 to 6 minutes. This negatively impacted the DX, as developers had to wait for all the checks to pass before they could commit any changes.\n\nHaving checks at the commit level also impacted the freedom to experiment without worrying about formatting guidelines or perfect type usage. Ultimately, what gets merged into production is what matters most.\n\nIf you're looking for arguments against pre-commit hooks, this serves as a good example.\n\n%[https://www.youtube.com/watch?v=RAelLqnnOp0] \n\nHashnode believes in moving fast and iterating rapidly but this was hurting our ability to move fast. We had to do something about it.  \nLet's take a look at how we improved our development workflow.\n\n## Optimizations to Speed Up Checks\n\nWe began considering ways to enhance our workflow, and several ideas emerged. One aspect we were certain about was the necessity to stop running these checks locally and transition everything to CI.\n\nWe need to take advantage of the tools at our disposal as much as we can and eliminate manual work. Let's talk about the improvements one at a time.\n\n### Moving Checks to CI\n\nWe already had a GitHub workflow in place for running tests on CI using GitHub actions; we expanded it to include type and lint checks.\n\nWe wanted to run these checks concurrently, so we utilized [jobs within the workflow](https://docs.github.com/en/actions/using-jobs/using-jobs-in-a-workflow) as a solution. This approach allowed us not to wait for one job to finish before starting another, enabling all three checks to run simultaneously. Let's examine the expanded workflow designed to execute these jobs concurrently.\n\n```yaml\nname: PR Validation\n\non:\n  pull_request:\n    types: [opened, reopened, synchronize, ready_for_review]\n    branches:\n      - development\n      - main\n\njobs:\n  cypress-run:\n    if: github.event.pull_request.draft == false\n    runs-on: ubuntu-latest\n    env:\n      NODE_ENV: test\n    steps:\n      #...steps\n  type-check:\n      #...checkout repo and install dependencies\n      - name: Type checking\n        run: tsc --no-Emit\n  lint-check:\n      #...checkout repo and install dependencies\n      - name: Linting\n        run: eslint . --fix --ext .js,.jsx,.ts,.tsx\n```\n\nNow, we have three jobs running concurrently for tests, lint errors, and type errors.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1691050575992/ffa86ea0-a32e-4c96-8ca9-186ce1101015.png align=\"center\")\n\nYou might be wondering, what about catching errors during development to prevent pushing them in the first place?\n\nOur IDEs are intelligent enough to detect these errors as we develop features, so we don't need to constantly check everything. The objective is to allow the merging of code only if it passes all checks in place, which can happen at the CI level.\n\nAfter moving all the checks to the CI, we removed Husky and developers were allowed to push as they deemed appropriate. We will simply block the merging of pull requests if any issues arise.\n\nThis was an improvement over the previous approach, but we still have more work to do.\n\nMachines running workflows are slower than our Macs, so checks performed on the CI are inevitably slower than when we run them locally.\n\nRunning all three checks on the CI took between 9 and 12 minutes. This meant that developers had to wait for 9 to 12 minutes before they could merge their pull requests. There is certainly room for improvement in this workflow.\n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">üöÄ</div>\n<div data-node-type=\"callout-text\">These checks happen at every commit as soon as you raise a pull request (which is not in draft). This workflow will run anytime a pull request is opened, reopened, synchronized or marked as ready for review for the development or main branches as base branch.</div>\n</div>\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1691493604969/6d51d96c-b8bd-44d7-bf5b-155545d01cb1.png align=\"center\")\n\n### Lint Staged Files in GitHub Actions\n\nOne quick improvement we could make is to run lint checks only for the files that have been changed, and that's where Lint-staged comes in handy.\n\nLint-staged typically works in conjunction with a pre-commit hook, such as Husky, to run lint checks on staged files only. However, we can modify it to run within a CI environment and focus solely on the files that have changed between commits. Let's replace the lint step in the workflow with this approach:\n\n```yaml\n- name: Linting\n   run: yarn lint-staged --diff=\"origin/${GITHUB_BASE_REF}...origin/${GITHUB_HEAD_REF}\" --no-stash\n```\n\nThis command calculates the difference between your branch and the base branch and then runs ESLint on it. This minor adjustment reduced the lint check duration from 30 seconds to 6 seconds.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1691050559924/c6cb39eb-7e47-4167-84f2-a3b3fb4ed947.png align=\"center\")\n\n### Skipping Library Check for TypeScript\n\nType checking, due to its nature, must run on the entire codebase, so there isn't much we can do to optimize it. However, we can skip the library check to make it slightly faster. To do this, replace the type check with the following:\n\n```yaml\n- name: Type checking\n   run: tsc --pretty --skipLibCheck --noEmit\n```\n\nNow that we've improved all the checks individually, it's time to cache whatever we can. Let's see how to cache the installation of dependencies for each job.\n\n### Caching Node Modules\n\nSince the three jobs run in parallel, they each need to install dependencies, which can be time-consuming. We've updated the workflow to cache dependencies and modified the \"Installing dependency\" step to skip when the cache is available. This can be achieved as follows:\n\n```yaml\n- name: Get yarn cache directory path\n    id: yarn-cache-dir-path\n    run: echo \"dir=$(yarn cache dir)\" >> $GITHUB_OUTPUT\n- uses: actions/cache@v3\n    id: yarn-cache\n    with:\n        path: |\n            **/node_modules\n            **/.eslintcache\n            ${{ steps.yarn-cache-dir-path.outputs.dir }}\n        key: ${{ runner.os }}-yarn-${{ hashFiles('**/yarn.lock') }}\n        restore-keys: |\n            ${{ runner.os }}-yarn-\n- name: Install dependencies\n    if: steps.yarn-cache.outputs.cache-hit != 'true'\n    run: yarn install --frozen-lockfile\n```\n\nWe include this for all jobs and retrieve `node_modules` from the cache when available. This significantly improved the time required to complete the entire check.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1691050813498/b16c02d4-f08b-4cfb-b071-4e249bc6a6d6.png align=\"center\")\n\nWe are already in good shape, but there is one more improvement we can add.\n\n### Removing Checks from Vercel\n\nVercel builds typically perform linting and type error checks by default. This causes the entire build process to take approximately 8 minutes.\n\nSince we already perform error checks on our end, there's no need to repeat them during builds. The final step is to bypass these checks in Vercel builds. We implemented this in `vercel.config.js`:\n\n```javascript\n  eslint: {\n    ignoreDuringBuilds: true,\n  },\n  typescript: {\n    ignoreBuildErrors: true,\n  },\n```\n\nYou can check [Vercel Docs](https://nextjs.org/docs/app/api-reference/next-config-js) for the config specification.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1691050886193/2e1e3ec1-cc4a-4247-9f07-161733b166a0.png align=\"center\")\n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">üíª</div>\n<div data-node-type=\"callout-text\">Developers have the freedom to set up pre-commit hooks locally if they prefer. If they believe it enhances their workflow, they should be allowed to do so. By shifting checks to the CI, we ensure that we don't impose any opinions on how developers should work, granting them the liberty to proceed as they see fit.</div>\n</div>\n\n## Conclusion\n\nBy adding all the necessary improvements we discussed above, we were able to reduce the time required to merge a pull request from 12 minutes to 5 minutes. This improved both the developer experience and the overall deployment process.\n\nHere's the summary of the improvements we added:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1690953115674/409ed5a5-4266-499a-86a5-f99e663eb941.png align=\"center\")\n\nI hope this article was informative and you learned something new today üòÑ.\n\nDo you think we can improve it further? Let us know in the comments."},"author":{"name":"Shad Mirza","profilePicture":"https://cdn.hashnode.com/res/hashnode/image/upload/v1663070035311/JaSbIMfve.jpg"},"coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/upload/v1691493522714/21672c1e-1f26-42a0-8ead-93f87f8211de.png"},"publishedAt":"2023-08-10T07:30:09.875Z","slug":"ci-checks-ensuring-better-code-quality-and-faster-deployment","brief":"How can you consistently deliver high-quality code that adheres to established coding guidelines and is free from errors?The solution lies in implementing tests and multiple checks for linting and type errors.\nThis may seem straightforward, but it re..."},{"id":"64b910642d0b5bc1eaea5e23","title":"Crafting Superior APIs with Design Guidelines: Set Up For Success","url":"https://engineering.hashnode.com/api-design-guidelines","content":{"markdown":"Building APIs ‚Äì especially public-facing ones ‚Äì is hard. There are a lot of decisions to be made, starting with the API concept (GraphQL, REST, etc.) to be used, infrastructure, and much more. But having this is just the foundation of your API, and you have to build a good API on top of it (whatever \"good\" means in your case). This is where an API design guideline can come into play.\n\nThis article is the first in a series of GraphQL-related articles. We will start with the importance of an API design guideline and how it could help you and your team to write a good API. It also should provide ideas on what to include in such a guideline.\n\nThis article won't cover the initially touched fundamental decisions (like REST vs. GraphQL) or the infrastructure it is running on. I'll proceed with GraphQL as an example because that is what we have chosen for our main API. This article is neither about implementations. It is about concepts and ideas.\n\n## API Design Guideline and Why It Is Important\n\n### What Is An API Design Guideline?\n\nFirst of all, what is \"**API design**\"? API design refers to the process of creating the interface that allows different software applications to communicate and exchange data with each other. Good API design can involve creating a well-structured, intuitive, and easy-to-use interface that meets the needs of its users while also being scalable, secure, and reliable. Again, how you define a \"good\" API design is depending on your needs.\n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">üí°</div>\n<div data-node-type=\"callout-text\"><strong>A good API design is depending on your own needs.</strong></div>\n</div>\n\nWhat about the term \"**guidelines**\"? Guidelines in general provide a set of best practices, conventions, and recommendations.\n\nBringing those definitions together: An **API design guideline** is a set of best practices, conventions, and recommendations for creating the interface that allows different software applications to communicate and exchange data with each other. These guidelines aim to promote good API design practices, ensure consistency, and improve the quality of the API.\n\n## GraphQL API Design Guideline On a High Level\n\nBefore diving into technicalities in your guideline, make sure to define the goals and non-goals of the guideline and how or by whom it will be updated. This helps to evolve the guideline and keep it focused. Speaking of keeping it focused: Make sure to get to the point and don't write lengthy documents. People won't read it or find the desired content otherwise.\n\nYou don't have to reinvent an API design guideline from scratch. You can stand on the shoulder of giants and get inspiration from other popular APIs ([Shopify](https://shopify.dev/api/storefront/2023-01/queries/product), [GitHub](https://docs.github.com/en/graphql)) etc. There are even design guidelines or tutorials out there like [Shopify GraqhQL Design Tutorial](https://github.com/Shopify/graphql-design-tutorial/tree/master).\n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">üí°</div>\n<div data-node-type=\"callout-text\"><strong>Stand on the shoulders of giants.</strong></div>\n</div>\n\nMake sure to include examples. Examples are easier to understand if you provide \"dos\" and \"don'ts\" ‚Äì and highlight them accordingly. This is what an example could look like (from our naming conventions section):\n\n```graphql\n‚ùå\ntype Publication {\n  # ...\n\t# We have a field called metaHTML in the database\n    # but it has no meaning to consumers\n  metaHTML: String\n}\n\n‚úÖ\ntype Publication {\n  # ...\n  descriptionSEO: String\n}\n\n‚ùå\ntype Address {\n  # How could the country look like? \"GER\", \"Germany\", \"üá©üá™\"? ü§î\n  country: String!\n}\n\n(‚úÖ)\ntype Address {\n  # Even this could mean ISO alpha-2 or alpha-3,\n  # but this could be clarified in a description.\n  countryCode: String!\n}\n```\n\nA few additional ideas that might improve your guideline document:\n\n* use toggles to hide some secondary content\n    \n* provide resources; where is the idea coming from?\n    \n* a short intro for a section on why it is important\n    \n* use examples from your domain\n    \n* KISS (Keep It Short Simple) ‚Äì nobody will read a book about your API\n    \n* highlight important keywords\n    \n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">ü§î</div>\n<div data-node-type=\"callout-text\">Any important things that you would add?</div>\n</div>\n\n## Impressions From Our Guideline\n\nIn this section, I will share a few impressions of our current GraphQL API design guideline. Maybe we will make it public at some point if we think it is good enough.\n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">‚Ñπ</div>\n<div data-node-type=\"callout-text\">Please note that our current public GraphQL API has been created before having those guidelines. The guidelines were (and are) used for our new, internal API that we will release to the public at some point.</div>\n</div>\n\nFirst of all, this is the table of contents of the current version of our GraphQL API design guide (*yes, we use Notion* üòú):\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1688734658201/9e98e1a5-5183-4c02-b572-d8768efd4408.png align=\"center\")\n\nI won't go into detail here but here are some things that our guideline includes. I'll copy some pieces, rephrase others, or provide ideas on what the section could include. Copied parts are shown as a quote.\n\n## General Naming Convention\n\n<div data-node-type=\"callout\">\n<div data-node-type=\"callout-emoji\">üòµ‚Äçüí´</div>\n<div data-node-type=\"callout-text\">‚ÄúThere are only two hard things in Computer Science: cache invalidation and naming things.‚Äù</div>\n</div>\n\n> Naming is a hard task. When a thing is named and being consumed by a client, there is almost no going back without a breaking change. To avoid this, try to be specific and avoid abstract names.\n> \n> Instead:\n> \n> * Choose field names based on **what makes sense**\n>     \n> * **Don‚Äôt be influenced** by legacy APIs or what the field is called in the database (DB Schema ‚â† GQL Schema)\n>     \n> * Choose **self-documenting** names\n>     \n>     Resources\n>     \n>     [Designing a scalable GraphQl schema](https://nordsecurity.com/blog/designing-a-scalable-graphql-schema)\n>     \n\n*Note: Here we are providing the examples that I provided in the previous section.*\n\n## Node Interface\n\n> Types that represent entities that have a lifetime (and are usually stored in their own database table) should implement the `Node` interface.\n> \n> Example:\n> \n> ```graphql\n> interface Node {\n>   id: ID!\n> }\n> \n> type Post implements Node {\n>   id: ID!\n>   # ...\n> }\n> ```\n\n## Booleans\n\n> Booleans are almost always non-nullable. Only make them optional if there is actual meaning in being `null` as opposed to `false`.\n> \n> If there are more states than two, consider using an `enum` instead.\n\n## Custom Scalars\n\nWhen should you create custom scalars? Unnecessarily introducing custom scalars can make your live harder evolving the API because you'll more likely introduce breaking changes.\n\nHow should they be named? We use PascalCase.\n\nNot use custom scalars for validation, e.g. `first_Int_NotNull_min_1_max_20`.\n\n## Queries ‚Äì Single Entities\n\n> * use the singular entity name, e.g. `post` (don‚Äôt use HTTP verbs like in `getPost`)\n>     \n> * if multiple **unique** filters are required (e.g. by `id` and `slug`) use a single query with inputs optional and throw an error if either no arguments or multiple provided\n>     \n> * use an **optional return value** to denote a ‚Äúnot found‚Äù rather than returning an error; only if not finding something really is an error it should be modeled as such (see `me` in examples)\n>     \n\n## Queries ‚Äì Multiple Entities\n\nThis section should answer questions such as:\n\n* How to name a query returning multiple entities?\n    \n* How to deal with pagination? When should pagination be added?\n    \n* How to deal with Filtering and Sorting?\n    \n\n## Mutations\n\nThis is another bigger section in our document and I'll just provide some ideas:\n\n* Naming convention: e.g. `publishPost` vs. `postPublish`\n    \n* We don't follow CRUD (Create, Read, Update, Delete), e.g. `createPost`, `updatePost` etc. We use more business-specific names and more fine-granular mutations.\n    \n* How to name arguments and outputs?\n    \n* What should you return? (Most often the whole entity, but that might not be the case for every mutation)\n    \n\n## Reading Recommendations\n\nThese are the reading recommendations that we have listed in our guideline:\n\n### Docs, Blog Posts, etc.\n\n* [Shopify GraphQL Design Tutorial](https://github.com/Shopify/graphql-design-tutorial/blob/master/TUTORIAL.md)\n    \n* [GraphQL Rules](https://graphql-rules.com/)\n    \n* [Designing GraphQL Mutations](https://www.apollographql.com/blog/graphql/basics/designing-graphql-mutations/)\n    \n* [GraphQL Best Practices](https://graphql.org/learn/best-practices/)\n    \n\n### Example Implementations\n\n* [Shopify Storefront API](https://shopify.dev/api/storefront/2023-01/queries/product)\n    \n* [Relay GraphQL Example Schema](https://github.com/relayjs/relay-examples/blob/880aee6b63573391916bf99948039ef53125804a/issue-tracker-next-v13/schema.graphql#L12381)\n    \n* [GitHub GraphQL API](https://docs.github.com/en/graphql)\n    \n\n### YouTube\n\n* [GraphQL Schema Design @ Scale (Marc-Andr√© Giroux)](https://www.youtube.com/watch?v=pJamhW2xPYw)\n    \n\n## Conclusion\n\nIn my opinion, an API design guideline is essential for a team with more than a few people to create and evolve an API that feels consistent, is easy to use, and maintainable.\n\nHopefully, this article gave you an idea of why an API design guideline is helpful and provided some ideas about what to include and how to write it.\n\nIs there anything that you think is a must in an API guideline?"},"author":{"name":"Jannik Wempe","profilePicture":"https://cdn.hashnode.com/res/hashnode/image/upload/v1686670777162/S3YVZnB5O.jpeg"},"coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/upload/v1689837535442/efafa971-ea06-485d-9d5c-d42a4f512e85.png"},"publishedAt":"2023-07-20T10:45:56.188Z","slug":"api-design-guidelines","brief":"Building APIs ‚Äì especially public-facing ones ‚Äì is hard. There are a lot of decisions to be made, starting with the API concept (GraphQL, REST, etc.) to be used, infrastructure, and much more. But having this is just the foundation of your API, and y..."},{"id":"6493ea8ff012983651c312d1","title":"Resolving High Disk Space Utilization in MongoDB","url":"https://engineering.hashnode.com/resolving-high-disk-space-utilization-in-mongodb","content":{"markdown":"### Problem\n\nWe have recently received many alerts about high disk usage on MongoDB Atlas for the past two weeks.\n\nAccording to docs, [Disk utilization % on Data Partition](https://www.mongodb.com/docs/atlas/reference/alert-resolutions/disk-io-utilization/) occurs if the percentage of time during which requests are being issued to any partition that contains the MongoDB collection data meets or exceeds the threshold. The downside of the Disk Utilization being high is the DB cannot process other queries if it maxes out. This can lead to data loss or inconsistencies.\n\nThese are how the metrics looked like in one of the last alerts. From the graph, Disk Util % at 4:30UTC was around 99.86%.\n\n![Max disk util % metric hitting maxing out at 100%](https://cdn.hashnode.com/res/hashnode/image/upload/v1687156481558/9c36fb63-e013-43cb-81da-082359d0038a.png align=\"center\")\n\nChecking the profiler, we can notice a query that's running during the same time, and it takes around `17s` to resolve!\n\n![a screenshot of a computer screen with a number of numbers on it](https://cdn.hashnode.com/res/hashnode/image/upload/v1682316589744/77e990f3-5a5e-4285-9e72-def7d2465829.png align=\"center\")\n\nThis query is related to a `KinesisAnalyticsLogs` collection. The collection internally is used to record views of different posts. The mentioned query already makes use of an index and still takes that much time to resolve because of the sheer size of the collection.\n\n![a screen shot of a web page with a number of items on it](https://cdn.hashnode.com/res/hashnode/image/upload/v1682312447723/bcf12123-229c-4247-bdf3-7bab64c757fa.png align=\"center\")\n\nThe total number of documents is close to 70 million, and the index size itself is close to a GB! That seems to be why it takes so long to resolve. Since this collection was recording analytical data, it was bound to reach this volume at some point. Along with that, from the profiler image, we can see that the query has yielded ~1300 times. According to [docs](https://www.mongodb.com/docs/manual/reference/database-profiler/#mongodb-data-system.profile.numYield), if a query yields a lot, then it is hitting a disk a lot. If we want that query to be faster, then the data needs to be in memory (index).\n\nUpon digging further, this query is scheduled to run every 30mins to sync the views. So we can correlate high query times on the profiler and Disk IOPS peaking almost simultaneously.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1682317743687/01f4c579-2d4a-4fec-bec3-9c3fec72bc76.png align=\"center\")\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1682317793584/30aa75ff-0132-4b1f-9547-e217f158b5f1.png align=\"center\")\n\n### Solutions\n\nBased on the investigation, we came up with two solutions:\n\n**Short Term Solution**\n\n* Since the collection size is a problem, and we are not using older data, deleting records older than a month will reduce the collection size drastically, leading to a smaller index size and faster query resolution.\n    \n* We can also add `TTL` to the records in `kinesisAnalyticsLogs` collection ([https://hashnode.com/rix/share/sg4lYYf\\_M](https://hashnode.com/rix/share/sg4lYYf_M)). It'll automatically delete the records older than a month going forward. This will make the index smaller and lead to a shorter query time.\n    \n\n**Long Term Solution**\n\nData like views/analytics should not be stored in the same place as the core business data. This collection will keep growing by the minute since it records the views. Some other DB should be used that's more appropriate for it.\n\n### Implementation\n\nWe decided to go with the short-term solution for now and added the long-term solution as a future task. For starters, we added TTL indexes immediately. With this, all the future records that will be created will be automatically deleted after the expiry time. This index can only be set on a field type `date`\n\n```javascript\nkinesisAnalyticsLogSchema.index({ dateAdded: 1 }, { expireAfterSeconds: 2592000 });\n```\n\nTo delete the past records, we ran a script that can delete all the records older than a month. Since we were deleting huge amounts of data within a short span, we encountered some problems while running the script. We had to keep a close eye on some of the metrics so that it didn't lead to a DB restart.\n\n* `CPU% spikes` A large number of record deletions were leading to CPU% usage over 95. We had to be careful and gave enough breathers in between to the DB.\n    \n* `Replication Oplog Window has gone below 1 hour` This was a new alert that we came across. Since our instance had one primary and two secondary DBs (replicas), the secondary DBs require enough time to replicate the writes from the primary. We had to be careful not to go below the recommended 1-hour window.\n    \n\nAfter carefully running the script, this is how the overall collection looked like\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1686812657442/60eba67a-1754-4e8f-8c9f-d6ff3f2bc919.png align=\"center\")\n\nThis was almost 2-3 days of effort to run the script and observe how the DB was performing. We finally were seeing the difference. The query resolution was fast enough for a background aggregate task, and it was not creating the disk util alerts üéâ\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1686817342070/33c9d255-4785-40b5-bdf1-5b5b9c4a02e9.png align=\"center\")\n\nOverall improvements:\n\n* Average query time went down to **1.5s** from **15s**\n    \n* The index size went down to **~400MB** from **~1GB**\n    \n* The collection size went down to **~9GB** from **~40GB**"},"author":{"name":"Vamsi Rao","profilePicture":"https://cdn.hashnode.com/res/hashnode/image/upload/v1665498938042/VRcVGyEcb.jpeg"},"coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/upload/v1686811735224/86d7f207-efe4-481c-a5d1-766a621cf262.png"},"publishedAt":"2023-06-22T06:30:39.500Z","slug":"resolving-high-disk-space-utilization-in-mongodb","brief":"Problem\nWe have recently received many alerts about high disk usage on MongoDB Atlas for the past two weeks.\nAccording to docs, Disk utilization % on Data Partition occurs if the percentage of time during which requests are being issued to any partit..."},{"id":"647ed1c80fce655a6be9ac07","title":"Simple Steps to Include CDK Diffs in GitHub PRs","url":"https://engineering.hashnode.com/simple-steps-to-include-cdk-diffs-in-github-prs","content":{"markdown":"Hey, everyone! If you've been using AWS, chances are you've come across CDK and building cloud apps with it. As seamless as it is to deploy apps using CDK, it is equally important to monitor changes in infrastructure code and prevent issues.\n\nThis guide is here to make your life a bit easier by showing you how to include the CDK diff directly in your GitHub PR.\n\nLet's start by understanding the CDK first.\n\n## A Brief Overview of CDK (Cloud Development Kit)\n\nThe Cloud Development Kit (CDK) enables developers to define, build, and deploy cloud infrastructure as code using supported programming languages. A CDK app consists of one or more Stacks where each stack defines AWS resources it uses, such as Lambda function, EC2 instance or SNS. This CDK app is usually written in TypeScript, JavaScript, Python, Java, C# or Go.\n\nA stack is simply a collection of AWS resources in the form of constructs managed as a single unit. You instantiate constructs within a stack to declare them to AWS and connect them using well-defined interfaces.\n\nHere's an example of using a construct in AWS CDK to create an Amazon S3 bucket with public read access:\n\n```typescript\nimport { Stack, StackProps, Construct } from '@aws-cdk/core';\nimport { Bucket } from '@aws-cdk/aws-s3';\n\nexport class MyS3BucketStack extends Stack {\n  constructor(scope: Construct, id: string, props?: StackProps) {\n    super(scope, id, props);\n\n    const bucket = new Bucket(this, 'MyS3Bucket', {\n      publicReadAccess: true,\n    });\n  }\n}\n```\n\nSince a construct is a reusable piece of code that encapsulates AWS resource creation and configuration logic, we can say that these constructs are the building blocks of a CDK app. Please pay close attention to this point, as it will be crucial in understanding why it is important to assess the CDK differences effectively.\n\n## Importance of CDK Diffs in GitHub PRs (Pull Requests)\n\nAs we just learned, a CDK app enables seamless building and deployment of cloud infrastructure as code. Since we define the resources used in an app as constructs, they can be employed to analyze the differences in resource allocation when changes occur.\n\nThese changes can be harmful if overlooked, leading to poor resource allocation, inconsistencies, and fatal crashes.\n\nAt times, it may not be apparent that changes could result in the deletion and recreation of a resource. For instance, in the case of a database, this could have disastrous consequences. That's why monitoring whenever there is a change in infrastructure code is essential.\n\nThe integration of CDK diffs in GitHub Pull Requests (PRs) makes it much easier for developers to review and analyze changes in the infrastructure code. This helps streamline the development process and ensure efficient cloud infrastructure management.\n\nA CDK diff highlights any modifications, additions, or deletions made to the codebase in a concise format. Since this information is available directly within the pull request, it can be leveraged to understand the proposed changes better, assess their impact, and make informed decisions. This, in turn, helps to minimize the risk of errors and maintain the stability and security of the cloud infrastructure, ultimately leading to a more reliable and robust system.\n\nI hope I've managed to convince you why having the CDK diff available in each pull request is crucial.\n\nNow, let's proceed to learn how to incorporate them.\n\n## Creating GitHub Workflow to Include CDK Diff as a PR Comment\n\nSince we need to add a comment with the CDK difference, we will use GitHub workflow to run a job whenever there is a new pull request. Create a `yaml` file inside `.github/workflows` and follow the steps below:\n\n> [Feel free to click here if you want to skip the explanation.](#heading-heres-the-complete-script)\n\n### Step 1: Add a name for the job and make sure it runs on pull requests only\n\n```yaml\n// .github/workflows/cdk-diff-share.yaml\nname: Run CDK difference\n\non:\n  pull_request:\n    branches: ['main']\n```\n\n### Step 2: Define a Job and Set the Environment, Permissions and Machine it Runs On\n\n```yaml\njobs:\n  build:\n    name: Run CDK diff\n    environment: prod\n    runs-on: ubuntu-latest\n    permissions:\n      id-token: write\n      contents: write\n      pull-requests: write\n```\n\nMake sure you have `contents` and `pull-requests` access so that the job can post the comment.\n\n### Step 3: Define Steps to Checkout the Repo and Install Dependencies\n\n```yaml\nsteps:\n      - name: Checkout repo\n        uses: actions/checkout@v3\n        with:\n          ref: ${{ github.event.pull_request.head.ref }}\n          repository: ${{ github.event.pull_request.head.repo.full_name }}\n      - name: Install pnpm\n        uses: pnpm/action-setup@v2\n        with:\n          version: 7.27.1\n      - name: Setup Node and Cache\n        uses: actions/setup-node@v3\n        with:\n          node-version: 16\n          cache: pnpm\n      - name: Install dependencies\n        run: pnpm install --prefer-offline\n```\n\nWe are also setting up the cache to speed up the consecutive runs.\n\n### Step 4: Configure AWS credentials\n\nWe need to be authenticated to access the AWS console; let's set up the AWS credentials using an IAM role.\n\n```yaml\n- name: Configure AWS credentials\n  uses: aws-actions/configure-aws-credentials@v1-node16\n  with:\n      role-to-assume: ${{ secrets.AWS_DEPLOY_ROLE }}\n      aws-region: <region-of-your-choice>\n```\n\nNote: Always use GitHub secrets to store any credentials.\n\n### Step 5: Run CDK sync\n\nRun the `CDK synth` command to generate a CloudFormation stack. We are using `aws-cdk` to run CDK commands.\n\n```yaml\n- name: CDK synth\n  run: pnpm cdk synth\n  env:\n     SKIP_SANDBOX: true\n```\n\n### Step 6: Run CDK diff\n\nRun the `CDK diff` command to compare the current stack with the deployed stack. Since we need this log in the next step to post a comment, we will also save this in a log file, say `output.log`.\n\nOnce we generate the log file, we can parse it and make it available as `GITHUB_OUTPUT`.\n\n```yaml\n- name: Run AWS CDK diff\n  run: pnpm cdk diff --app \"./cdk.out/assembly-ProdStage\" 2>&1 2>&1 | tee output.log\n   env:\n      SKIP_SANDBOX: true\n- name: Echo output log\n  id: output_log\n  run: |\n     echo \"data<<EOF\" >> $GITHUB_OUTPUT\n     echo \"$(cat output.log)\" >> $GITHUB_OUTPUT\n     echo \"EOF\" >> $GITHUB_OUTPUT\n```\n\nHere, `2>&1 2>&1 | tee output.log` redirects both stdout and stderr to `output.log` file and also prints them on the console\n\n### Step 7: Post the comment\n\nWe are using `mshick/add-pr-comment@v2` to post a log on the GitHub pull request. We will add a markdown wrapper around the log to make it appear visually appealing.\n\n```yaml\n- name: Post diff in comment\n  uses: mshick/add-pr-comment@v2\n  with:\n    message-id: cdk-diff\n    message: |\n      #### CDK Diff for ProdStage\n      <details>\n         <summary>Show diff</summary>\n         ` ` `bash\n         ${{ steps.output_log.outputs.data }}\n         ` ` `\n      </details>\n```\n\nThat's we are done. Here's how the log should appear:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1682942481140/7026eab1-1cf9-460c-97e7-6495d65228d2.png align=\"center\")\n\n### Here's the complete script:\n\n```yaml\nname: Run CDK difference\n\non:\n  pull_request:\n    branches: ['main']\n\njobs:\n  build:\n    name: Run CDK diff\n    environment: prod\n    runs-on: ubuntu-latest\n    permissions:\n      id-token: write\n      contents: write\n      pull-requests: write\n    steps:\n      - name: Checkout repo\n        uses: actions/checkout@v3\n        with:\n          ref: ${{ github.event.pull_request.head.ref }}\n          repository: ${{ github.event.pull_request.head.repo.full_name }}\n      - name: Install pnpm\n        uses: pnpm/action-setup@v2\n        with:\n          version: 7.27.1\n      - name: Setup Node and Cache\n        uses: actions/setup-node@v3\n        with:\n          node-version: 16\n          cache: pnpm\n      - name: Install dependencies\n        run: pnpm install --prefer-offline\n      - name: Configure AWS credentials\n        uses: aws-actions/configure-aws-credentials@v1-node16\n        with:\n          role-to-assume: ${{ secrets.AWS_DEPLOY_ROLE }}\n          aws-region: <your-region-here>\n      - name: CDK synth\n        run: pnpm cdk synth\n        env:\n          SKIP_SANDBOX: true\n      - name: Run AWS CDK diff\n        run: pnpm cdk diff --app \"./cdk.out/assembly-ProdStage\" 2>&1 2>&1 | tee output.log\n        env:\n          SKIP_SANDBOX: true\n      - name: Save output\n        id: output_log\n        run: |\n          echo \"data<<EOF\" >> $GITHUB_OUTPUT\n          echo \"$(cat output.log)\" >> $GITHUB_OUTPUT\n          echo \"EOF\" >> $GITHUB_OUTPUT\n      - name: Post diff in comment\n        uses: mshick/add-pr-comment@v2\n        with:\n          message-id: cdk-diff\n          message: |\n            #### CDK Diff for ProdStage\n            <details>\n              <summary>Show diff</summary>\n              ` ` `bash\n              ${{ steps.output_log.outputs.data }}\n              ` ` `\n            </details>\n```\n\n> Please remove the space between backticks \\`.\n\n## **Conclusion**\n\nCDK diff in GitHub PRs helps us review and analyze changes in infrastructure code, minimizing the risk of errors and maintaining stability and security. CDK diffs highlight modifications, additions, or deletions in the codebase, enabling a better understanding of proposed changes and informed decision-making.\n\nWe learned how to utilize `cdk diff` it to see changes in the intended deployment and post them as a comment on our GitHub PRs.\n\nDo let us know if this article was helpful in the comments."},"author":{"name":"Shad Mirza","profilePicture":"https://cdn.hashnode.com/res/hashnode/image/upload/v1663070035311/JaSbIMfve.jpg"},"coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/upload/v1686032625812/72664174-525d-44e6-bcc1-94ccd855561f.png"},"publishedAt":"2023-06-06T06:27:20.956Z","slug":"simple-steps-to-include-cdk-diffs-in-github-prs","brief":"Hey, everyone! If you've been using AWS, chances are you've come across CDK and building cloud apps with it. As seamless as it is to deploy apps using CDK, it is equally important to monitor changes in infrastructure code and prevent issues.\nThis gui..."},{"id":"646de0479098493346f6fff5","title":"Keep parts of your GraphQL Introspection Query hidden","url":"https://engineering.hashnode.com/keep-parts-of-your-graphql-introspection-query-hidden","content":{"markdown":"Once you have created your first schema and your GraphQL server is up and running, it makes sense to implement precautions to prevent it from being compromised by bad actors. In the context of Hashnode, our GraphQL API serves our website and blogs. This article will explore excluding fields from an introspection request without disabling the server's discoverability feature by completely turning off the introspection queries.\n\n## The Result of Hiding Parts of the Schema on Introspection\n\nThe diagram below shows the final result of using a directive and an Apollo Server plugin to hide aspects from the incoming introspection request.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1684326033578/2eca5989-8763-4177-a1d9-65582e07d155.png align=\"center\")\n\nWe have a schema defined where multiple fields are annotated with `@hidden`. In the returned resultset of the Introspection query, we can see that all those fields are missing.\n\nLet's explore how we implemented it!\n\n## Taking a Look at GraphQL Schema Introspection\n\nGraphQL is a strongly typed query language. The schema, exposed from a GQL API, acts as a contract between the frontend and backend and defines the available operations. The server knows about the schema, as it's the place of definition. But how does the frontend get to know about the schema?\n\nThis is where one of the key features of GraphQL comes in hand: Schema introspection. Offering the introspection of a schema allows the clients to explore and learn about what's possible in a given API in terms of queries, mutations, subscriptions, fields, and types.\n\nThere are a couple of keywords to request schema introspections. A double underscore precedes all of them: `__`. A simple introspection query for all available types could look like this:\n\n```graphql\nquery {\n  __schema {\n    types {\n      name\n    }\n  }\n}\n```\n\nSounds great, right? It is, you get a kind of self-documentation for free. But what if you don't want to scream out your **whole** schema to the world? Due to the nature of GQL and the contract a schema poses, everything in your GraphQL schema will be visible to the world.\n\nHow can we hide certain aspects of our schema?\n\n* Shutting down the Introspection Query is a certain way to keep things private. This might be an option in a private API, but why would we hide the schema there? Turning off the introspection query in a public API would cause all our consumers to lose the ability to discover the schema, which is certainly not what we want. Additionally, automatic code generation is a common use case that relies on introspection.\n    \n* Removing certain fields from the introspection is another approach that may be more complex to manage but does not break every third-party consumer.\n    \n\nLet's explore how we can solve this problem by using some native GQL features as well as a specialty of Apollo Server: directives and plugins üé¨\n\n## Directives in Graphql Can Apply Custom Logic\n\nA directive in GraphQL is a decorator for parts of a schema or operation. An `@` character always precedes it. The most common and build-in directive is the `@depreacted` decorator that indicates the deprecation for a field:\n\n```graphql\ntype ExampleType {\n  oldField: String @deprecated(reason: \"Use `newField`.\")\n  newField: String\n}\n```\n\nThis shows two things:\n\n1. A directive can take an argument, e.g., `reason`.\n    \n2. A directive will always be placed after declaring a field or operation.\n    \n\nThere are multiple valid locations for a directive. For example, a directive declared with `ARGUMENT_DEFINITION` can be placed on an argument but not somewhere else within the schema.\n\n```graphql\ntype ExampleType {\n  oldField: String @argumentOnlyDirective # would throw an error \n  newField: String\n}\n```\n\nOverall, directives are neet enhancements for GQL that allow the execution of custom logic as appropriate. Things like authentication or authorization can be built easily with a directive.\n\nFor further reading, check out the [Apollo Server docs on directives](https://www.apollographql.com/docs/apollo-server/schema/directives/).\n\n## Formalizing the `@hidden` Directive\n\nIn our case, we want a directive that can be used in different places within our schema to hide things from the Introspection output. The simplest form of our directive can look something like this:\n\n```graphql\n  directive @hidden on OBJECT | FIELD_DEFINITION\n```\n\nWe want to be able to hide any `OBJECT`or `FIELD_DEFINITION`. This allows removing of various things from the schema. This can be enhanced to your own needs by adding additional valid locations.\n\n```graphql\ntype Example {\n fieldA: String!\n fieldB: String! @hidden\n} \n\ntype Query {\n getExampleOne: Example!\n getExampleHidden: Example! @hidden\n} \n\ntype Mutation {\n mutateOnExampleHidden(input: String!): Example! @hidden\n}\n```\n\nConsidering the above schema definition, our `@hidden` directive should be able to transform the schema once an introspection is requested by removing all the fields from the resultset, which are declared with the directive. The visible schema for a requesting consumer should look like this:\n\n```graphql\ntype Example {\n fieldA: String!\n} \n\ntype Query {\n getExampleOne: Example!\n}\n```\n\nCoding this out should be rather straightforward. If we find the annotation `@hidden`, we can remove the declared fields from the schema; otherwise, we return the original type.\n\n```typescript\nconst directive = getDirective(...);\n  \nif (directive) {\n return null;\n} else {\n return type;\n}\n```\n\n## Why Does the Directive Alone Not Work?\n\nExcellent, we now have a definition of our new directive and an Idea of how we want the output to be transformed. Once implemented, we can observe a weird behavior: It works on introspection but will also remove fields from the actual GraphQL results sets on incoming requests ü§Ø\n\nWhy is that?\n\nRetake a look at how we defined the directive from a custom logic point of view and consider when directives are applied.\n\nIn Apollo Server, a directive is applied by transforming the schema. So, applying the directive on server startup with the above code would remove the annotated fields from inspection and the whole schema. This is not what we want. The context provided to the transform function does not indicate the query type. Furthermore, introspection queries are handled a little differently by the Apollo Server.\n\nHow can we still achieve our goal of hiding things from the introspecting only?  \n‚û°Ô∏è Apollo plugins to the rescue üöÄ\n\n## Plugins in Apollo Server Allow You to Hook Into the Request Life Cycle\n\nApollo allows you to implement server plugins to perform custom operations in response to specific events. A plugin is a JavaScript object that implements one or more functions responding to events. Within a plugin are two major event categories: server [lifecycle events](https://www.apollographql.com/docs/apollo-server/integrations/plugins-event-reference#server-lifecycle-events) and [request lifecycle events](https://www.apollographql.com/docs/apollo-server/integrations/plugins-event-reference#request-lifecycle-events). In our case, we are interested in Request lifecycle events, as we want to respond to a certain event within the execution of an operation, e.g. a Request to our API.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1684155854681/a15bf2fc-94c6-4a0d-979a-d8522b72e651.png align=\"center\")\n\nTaking the above flow of the request lifecycle into account, we can see an option to hook into the returned result before the actual execution starts, in that we write a plugin that will provide its result within the `responseForOperation` hook.\n\nTaking a look into the definition of the hook, we can also confirm this above observation\n\n> The `responseForOperation` event is fired immediately before GraphQL execution would take place. If its return value resolves to a non-null `GraphQLResponse`, that result is used instead of executing the query. Hooks from different plugins are invoked in series, and the first non-null response is used.\n\nIn this hook, we have various information at our disposal to decide if we want the normal resolver flow to be invoked or if we want to do a kind of *early exit* without resolving the given fields:\n\n```typescript\nresponseForOperation?(\n requestContext:WithRequired<GraphQLRequestContext<TContext>,\n 'source' | 'queryHash' | 'document' | 'operationName' |'operation'\n): Promise<GraphQLResponse | null>;\n```\n\nWithin the `requestContxt` we find the information about which `query` was sent to the server and the operation name, and we get our hands on the whole schema that will be used for executing the request.\n\nLet's use this to combine our custom directive with a custom plugin that will hide aspects of our schema from introspection requests.\n\n## An Implementation Tells More Than 1000 Words!\n\nOverall, we have now discovered a couple of things we can make use of:\n\n* Our directive will transform a schema and remove everything annotated with `@hidden`\n    \n* We can hook into the request execution lifecycle by providing a custom plugin\n    \n* `responseForOperation` that allows us to provide a different result and skip the normal resolver execution for a request\n    \n* Within the `requestContext` we have all the information, which is also available for the normal execution of the request, to generate our result for a request\n    \n\nThe main idea is to hook into the request execution and check if we are receiving an introspection of the schema. With the information provided by `responseForOperation` and the option to directly provided a result here, we can skip the actual execution and provide a different result. This is where we can apply our hidden directive to transform the schema and remove everything annotated with `@hidden`. We will now use this updated schema **only** if we receive an introspection for further execution of the request.\n\nLet's take a look at the implementation of the plugin and directive!\n\n### The `@hidden` Directive\n\nThe directive consists of the code snippets from above and some more checks. The result of the function we are defining here is a `transformer` that will change the schema. It takes a GraphQL schema as input and will return one.\n\n```typescript\nimport { getDirective, MapperKind, mapSchema } from '@graphql-tools/utils';\nimport { gql } from 'graphql-tag';\nimport { GraphQLSchema } from 'graphql';\n\ntype DirectiveTransformer = (schema: GraphQLSchema) => GraphQLSchema;\n\n\nexport const hiddenDirectiveTypeDefs = gql`\n  directive @hidden on OBJECT | FIELD_DEFINITION\n`;\n/**\n * This directive transformer removes all types and fields that are marked with the @hidden directive from introspection queries only\n */\nexport const hiddenDirectiveTransformer: DirectiveTransformer = (schema) => {\n  const directiveName = 'hidden';\n\n  return mapSchema(schema, {\n    [MapperKind.TYPE]: (type) => {\n      const directive = getDirective(schema, type, directiveName)?.[0];\n      if (directive) {\n        return null;\n      } else {\n        return type;\n      }\n    },\n    [MapperKind.OBJECT_FIELD]: (fieldConfig) => {\n      const directive = getDirective(schema, fieldConfig, directiveName)?.[0];\n      if (directive) {\n        return null;\n      } else {\n        return fieldConfig;\n      }\n    }\n  });\n};\n```\n\nThe above code will take the schema and find all occurrences of `hidden`. Then, we check if it's defined in the right place and apply our custom logic to it. The logic will either remove the `fieldConfig` or the `type` from the schema if the directive was found. Otherwise, it will return the original definition and not change the schema.\n\n### The Apollo Server Plugin\n\nOur server plugin will now use `requestDidStart` lifecycle and the `responseForOperation` event. We need a way to identify if we are receiving an introspection query. For this, let's add a simple regex to check if the query contains at least one of the introspection keywords.\n\n```typescript\nconst REGEX_INTROSPECTION_QUERY = /\\b(__schema|__type|__typename)\\b/;\n```\n\nWe can now identify if we receive an introspection request to our schema or if it's a normal operation without any introspection parts.\n\nSo, let's bring everything together and write our custom plugin to apply our directive once we receive a schema introspection:\n\nFirst, we want to check if the plugin is being executed in the `production` environment in `development`we want full access to the schema and the playground. By returning `null`we can tell Apollo that we don't want to provide our own result for the specific query, and the execution flow should be run normally.\n\n```typescript\n// only on production\nif (!isProd) return null;\n```\n\nSecondly, we check if the query is an actual introspection request by checking the operation name (playgrounds often send a named query called `IntrospectionQuery` to the sever) or if it includes any reserved keywords that are part of the schema introspection.\n\n```typescript\nconst isIntrospectionQuery =\n ctx.request.operationName === 'IntrospectionQuery' ||\n ctx.request.query?.includes('IntrospectionQuery');\n\nconst hasIntrospectionParts =\n isIntrospectionQuery ||\n REGEX_INTROSPECTION_QUERY.test(ctx.request.query || '');\n```\n\nIf we determine this to be true, we take the original schema and pass it to the `hiddenDirectiveTransformer,` which will clean our schema from everything we don't want to include within the introspection result.\n\n```typescript\nconst schema = hiddenDirectiveTransformer(ctx?.schema);\nconst result = await execute({\n schema: schema,\n document,\n contextValue: ctx.contextValue,\n variableValues: request.variables,\n operationName: request.operationName\n});\n```\n\nThe last step is executing the actual introspection request by calling the `execute` function provided by the `graphql` package.\n\nNow we can create the response to the request by copying everything from the initial request and overriding the body:\n\n```typescript\nconst response: GraphQLResponse = {\n ...ctx.response,\n body: {\n  kind: 'single',\n  singleResult: {\n   ...result\n  }\n }\n};\n```\n\nAnd that's it. Piercing every part together, our plugin now looks like this:\n\n```typescript\ntype PluginDefinition = ApolloServerPlugin<Context>;\n\nexport const introspectionPlugin: (isProd: boolean) => PluginDefinition = (\n  isProd\n) => ({\n  async requestDidStart() {\n    return {\n      // This event is fired immediately before GraphQL execution \n      //takes place\n      async responseForOperation(ctx) {\n        // only on production\n        if (!isProd) return null;\n\n        const isIntrospectionQuery =\n          ctx.request.operationName === 'IntrospectionQuery' ||\n          ctx.request.query?.includes('IntrospectionQuery');\n\n        const hasIntrospectionParts =\n          isIntrospectionQuery ||\n          REGEX_INTROSPECTION_QUERY.test(ctx.request.query || '');\n        // If it's an introspection query, we need to apply the hidden \n        // directive transformer ourself\n        // otherwise, let Apollo handle the request by returning null\n        if (hasIntrospectionParts) {\n          const { request, document } = ctx;\n\n          // APPLY @hidden\n          const schema = hiddenDirectiveTransformer(ctx?.schema);\n          // Executing the request \n          const result = await execute({\n            schema: schema,\n            document,\n            contextValue: ctx.contextValue,\n            variableValues: request.variables,\n            operationName: request.operationName\n          });\n\n          // Setting the result\n          const response: GraphQLResponse = {\n            ...ctx.response,\n            body: {\n              kind: 'single',\n              singleResult: {\n                ...result\n              }\n            }\n          };\n          return response;\n        }\n        return null;\n      }\n    };\n  }\n});\n```\n\nNow we can apply our directive type definitions to our base schema and add our plugin to the Apollo Server - let's see how it works out üëÄ\n\n### Let‚Äôs Test It Out üöÄ\n\nTo check if everything works as expected, we will again use our simple schema to verify our implementation.\n\n```graphql\ntype Example {\n fieldA: String!\n fieldB: String! @hidden\n} \n\ntype Query {\n getExampleOne: Example!\n getExampleHidden: Example! @hidden\n} \n\ntype Mutation {\n mutateOnExampleHidden(input: String!): Example! @hidden\n}\n```\n\nLet's request a simple introspection query that checks for all fields on `query` and `mutation` as well as specifically requesting the `Example` type from our schema.\n\n```graphql\nquery {\n  __schema {\n    queryType {\n      name\n      fields {\n        name\n      }\n    }\n    mutationType {\n      name\n      fields {\n        name\n      }\n    }\n  }\n  __type(name: \"Example\") {\n    fields {\n      name\n    }\n  }\n}\n```\n\nRunning this in our `development` environment will return the following result:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1683896081794/c38c8cb5-3c03-45cf-a2bc-7342a861f535.png align=\"center\")\n\nAll the fields are there, even though some are annotated with `@hidden` as we are not running in production.\n\nNow repeat this in our `production` environment, and we see the following result:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1683896198367/7ac9a738-055b-40c8-b8fc-e3b46c74f710.png align=\"center\")\n\nYeah üéâ We have successfully omitted the fields we don't want to be part of a public introspection request to our server ü™Ñ\n\n## Takeaways\n\nNow let's revisit what we implemented here and what implications we have.\n\nFirst, this is, by no means, a safeguard for your server. Even if we omit certain fields and types from the introspection response, they are still part of the schema and will be available normally. Rather see it as making it a little harder and not as obvious for your consumers that some `private` things exist in your schema. Overall, the best way of thinking is always that everybody can see everything in GraphQL, and you should design your schema and security following this thought.\n\nSecondly, you must ensure that any reference to other definitions you have made must also be hidden. Take a look at the following code snippet:\n\n```graphql\ntype ThisIsHidden @hidden {\nfieldOther: String!\n}\n\ntype Example {\n fieldA: String!\n fieldB: String!\n fieldC: ThisIsHidden!\n} \n\ntype Query {\n getExampleOne: Example!\n getExampleHidden: Example! @hidden\n} \n\ntype Mutation {\n mutateOnExampleHidden(input: String!): Example! @hidden\n}\n```\n\nIf we now make an introspection query, this will fail, as we have removed the definition of `ThisIsHidden` from our schema but still referring to it in the `Example` type. Some third-party tools may break due to the missing reference. An easy fix is to add: `fieldC: ThisIsHidden! @hidden`. Still, you need to take care of this yourself.\n\nAlways double-check if you are not breaking other tools that are relying on your schema by carelessly using the hidden directive üôå\n\nAlright, that's it about hiding things from Introspection queries to your Server - I hope you enjoyed the read üëã"},"author":{"name":"Florian Fuchs","profilePicture":"https://cdn.hashnode.com/res/hashnode/image/upload/v1675085454170/766f4dd9-9fba-4aa7-b141-ff62bba7eafe.jpeg"},"coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/upload/v1683820367152/f890c01d-283f-4a00-acad-34bc2d2542e4.png"},"publishedAt":"2023-05-24T10:00:39.246Z","slug":"keep-parts-of-your-graphql-introspection-query-hidden","brief":"Once you have created your first schema and your GraphQL server is up and running, it makes sense to implement precautions to prevent it from being compromised by bad actors. In the context of Hashnode, our GraphQL API serves our website and blogs. T..."},{"id":"645bb1927c65daa656f54d96","title":"How Hashnode is using Rate Limits on Stellate","url":"https://engineering.hashnode.com/how-hashnode-is-using-rate-limits-on-stellate","content":{"markdown":"Rate Limits are a vital part of every API. Especially, since we operate quite a lot of public and unprotected APIs we need to rate limit them.\n\nImplementing Rate limits based on IP addresses is fairly easy, especially with Amazon API Gateway and Amazon WAF. But limiting requests based on other user identifiers such as user IDs in JWTs or authorization headers can get quite tricky.\n\nThat is where Stellate comes to the rescue ü¶∏üèΩ\n\n## Our Architecture\n\nA small primer to our architecture and all involved parts:\n\n![Overall Architecture of Hashnode](https://cdn.hashnode.com/res/hashnode/image/upload/v1682692856042/5d6a29e2-7910-4898-ba13-e5c376865343.png align=\"center\")\n\nWe have two methods of accessing our API: either through **client calls** or via **server-side-rendered** calls from Vercel. Both pass through Stellate's Edge Cache and now also utilize their rate-limiting feature. Following this, we make calls to the API Gateway on AWS.\n\n## Why not only IP?\n\nThe first question we mostly got about rate limits is: **Why don't you only rate limit on IP?**\n\nWhile it makes sense to rate limit on the IP address it often is a misleading identifier. Due to the shortage of IPs, many mobile carriers or internet providers share the IP across several zones. Also, universities, dorms, and companies are often using the same IP. If we would rate limit this one IP the whole university couldn't access Hashnode anymore. This is not what we want.\n\n## Why Rate Limits?\n\nThere are mainly two reasons why we need rate limits.\n\n### Impact on the Database\n\nFirst of all, we want to secure our database. You don't want people to be able to hit your database constantly. This will incur costs and can lead to downtime.\n\nYes, caching is the number one thing to consider here. Stellate & Vercel are helping us with that already. But rate limits also help by ensuring that nobody bombards your API. Everything that isn't cached (e.g. `Mutations` ) hits the DB directly. We want to avoid that.\n\n### Impact on your Business\n\nThe second reason is, you simply don't want that your product gets abused. We don't want an automatic generation of posts running in a script.\n\nWe need rate limits to ensure that nobody abuses our API and impacts the database.\n\n## Stellate Rate Limiting\n\n[Stellate](https://hshno.de/FEuPQqw) is a CDN for GraphQL. It mainly offers the functionality of:\n\n1. Caching GQL Requests on the Edge\n    \n2. Analytics and errors about your API\n    \n3. **Rate Limiting**\n    \n\nThe rate-limiting feature is currently in its public beta phase. Check out their [docs](https://stellate.co/docs/graphql-rate-limiting) for more information.\n\nTo enable rate limits you can simply add the `rateLimits` field to your Stellate config file (with TypeScript support! üòâ):\n\n```typescript\nimport { Config } from 'stellate'\n\nconst config: Config = {\n  config: {\n    rateLimits: [\n      {\n        name: 'IP limit',\n        groupBy: 'ip',\n        state: 'dryRun',\n        limit: {\n          type: 'RequestCount',\n          window: '1m',\n          budget: 50,\n        },\n      },\n    ],\n  },\n}\nexport default config\n```\n\nThe code above creates a rate limit of 50 requests for every minute. The state `dryRun` means that this rate limit is not really available. Your dashboard will only show you which requests **would be blocked** but they won't be blocked.\n\n### Dry Run\n\nThe dry run mode in Stellate is an excellent feature for gaining a better understanding of the appropriate rate limit. Once you've activated it you can head over to your Stellate Dashboard, check the rate limiting dashboard, and see how many requests and customers would have been blocked. But no request will be blocked.\n\n![Dashboard for rate limits in Stellate](https://cdn.hashnode.com/res/hashnode/image/upload/v1683730304530/43f124fa-0c60-45cd-ad65-349c519b54c1.png align=\"center\")\n\nYou can also send requests from the Stellate playground or from the API Client of your choice (cURL, Postman, Insomnia) and check the remaining budget.\n\nIn this example, I query my blog. In the result window on the right, I can see that the rule \"Unatuehtnicated IP Limit - Request Count\" was applied. I have 1998 of 2000 requests remaining.\n\n![a screenshot of a computer screen with a code on it](https://cdn.hashnode.com/res/hashnode/image/upload/v1683730422729/8174681d-0f43-4ab7-84bd-8718c7c40f7d.png align=\"center\")\n\n## Rate Limits\n\nSo far the introduction. But how do we use rate limits at Hashnode? We distinguish mainly from two different limits:\n\n1. Authenticated access\n    \n2. Unauthenticated access\n    \n\n### Authenticated Access\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1682692491128/f9e5c284-f05a-4b31-b94b-b7864fef5352.png align=\"center\")\n\nAuthenticated access is everything where a token in a cookie or header is present. If this token is present we create a limit of 500 requests per minute.\n\n```typescript\n{\n  name: 'Authenticated User Limit - Request Count',\n  groupBy: req.headers['token'],\n  state: 'enabled',\n  limit: {\n    type: 'RequestCount',\n    budget: 500,\n    window: '1m'\n  }\n}\n```\n\nThis defines that each header `token` will have a limit of 500 requests per minute. To test this out you can also make use of Stellate's dashboard.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1682686423313/c4a810dd-7ca7-4708-94e2-e0ff09c4588e.png align=\"center\")\n\nHere we query my personal blog and access the `title`. At the bottom of the result, we can see the remaining limit. In this case, we have 499 of 500 requests left.\n\n### Unauthenticated Access\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1682692472212/9ce5d2c0-7b0b-4f76-a9d2-3859d67978bf.png align=\"center\")\n\nUnauthenticated access, on the other hand, is everything without an authentication token. In this case we group by the IP address of the user. This limit has 2000 requests per minute.\n\n**Why is this limit larger?**\n\nFirst of all, unauthenticated requests are typically *cheaper* in terms of computational costs. Querying a blog vs. creating a blog is a huge difference.\n\nThe second reason is the reason of IP sharing. We saw a lot of cases in that IPs are shared. If this is the case we don't want to have a too tight budget. This is why we allow quite a bit more room for unauthenticated access.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1682686578720/b13b1d39-bc37-444e-92a1-2247ef026486.png align=\"center\")\n\n## Rate Limits & Server-Side Rendering\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1682692505588/6c5f97dd-f5b1-44af-ae8a-487ac4da319a.png align=\"center\")\n\nHashnode makes heavy usage of Vercel and Server-Side Rendering (SSR). The problem with SSR & Rate limiting is that many customers can visit blogs that will be server-side rendered from the same server. This will then come from the same IP address.\n\nThere are separate solutions to take care of that:\n\n1. Ignore SSR for rate limits\n    \n2. Forward the public IP & authorization header (if present) to Stellate.\n    \n3. (New) With [Vercel Secure Compute](https://hshno.de/jRSvsUN) assign a fixed IP to Vercel and whitelist this one\n    \n\nWe opted for the first solution, ignoring all SSR calls. We primarily chose this option because we wanted to address rate limiting for the API. This is also a preparatory step for making our API publicly available. It is not specifically intended to rate limit the client's usage.\n\nYou can do that by defining a secret between Vercel & Stellate. This secret can for example be a header you'll forward to each API Requests.\n\n> ‚ö†Ô∏è Be aware that this header needs to be treated as a secret. You can only send it from the server side **not** from the client side.\n\nIn Stellate you can then define the following:\n\n```typescript\n      if (\n        req.headers['ssr-call'] &&\n        req.headers['ssr-call'] === \"123\"\n      ) {\n        return [];\n      }\n```\n\nThis will return no rate limit in case the call is coming from Vercel.\n\n## Block IPs\n\nOne more remarkable feature is the ability to block individual IP addresses. Unfortunately, we face attacks quite frequently. Often, these attacks originate from a single IP address. Blocking such an IP address using rate limits is incredibly simple:\n\n```typescript\n      if (ipListToBlock.includes(req.ip)) {\n        return [\n          {\n            name: 'Blocked IP limit',\n            groupBy: 'ip',\n            state: 'enabled',\n            limit: {\n              type: 'RequestCount',\n              budget: 0,\n              window: '1m'\n            }\n          }\n        ];\n      }\n```\n\n## That's it üéâ\n\nThat is all about rate limits.\n\nIt already saved us from a huge abusive spike in traffic to our API and it is super easy to implement!"},"author":{"name":"Sandro Volpicella","profilePicture":"https://cdn.hashnode.com/res/hashnode/image/upload/v1627027343841/bT6HIivZm.jpeg"},"coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/upload/v1683620384065/c2fc7e7d-8803-43b5-8fe9-fb80adb13552.png"},"publishedAt":"2023-05-10T15:00:34.500Z","slug":"how-hashnode-is-using-rate-limits-on-stellate","brief":"Rate Limits are a vital part of every API. Especially, since we operate quite a lot of public and unprotected APIs we need to rate limit them.\nImplementing Rate limits based on IP addresses is fairly easy, especially with Amazon API Gateway and Amazo..."},{"id":"643f836df032a1495fab9abe","title":"Custom Metrics Made Easy: A Comprehensive Guide to SST and CloudWatch Integration","url":"https://engineering.hashnode.com/custom-metrics-made-easy-a-comprehensive-guide-to-sst-and-cloudwatch-integration","content":{"markdown":"Monitoring and managing resources is an important part of making sure your serverless application is working as expected and troubleshooting when issues arise. Amazon Web Services (AWS) provides a wide range of services to monitor and track the performance of your resources. One of these services is CloudWatch, which allows you to collect and track metrics, collect and monitor log files, and set alarms for all the AWS Services.\n\nThis article will discuss such metrics, specially custom metrics in depth. We will also understand when they can be helpful, and how to add them to using SST. At last, we will look at how we use them at Hashnode.\n\nLet's start by understanding the platform [Serverless Stack (SST)](https://sst.dev/) and how it helps in developing serverless applications.\n\n## What is SST\n\n[SST (Serverless Stack)](https://sst.dev/) simplifies developing and deploying AWS applications. SST's construct makes it easy to add features with very few lines of code.\n\nThe `create-sst` CLI helps us create a new SST app with a basic structure. The `cd` command changes the current directory to the new app and `npm install` will install the required dependencies.  \nFinally, you can run `sst start` to deploy your sandbox environment to AWS and start your debug stack.\n\n```bash\nnpx create-sst@latest my-sst-app\n\ncd my-sst-app\nnpm install\n\nnpx sst dev\n```\n\nThe CLI will ask you to pick a stage name which can be your name. Once the setup is complete, the app will be deployed to AWS and connected to the local machine. You can then jump to the SST console [https://console.sst.dev/](https://console.sst.dev/) which will stream all the logs.\n\n## What are Custom Metrics and When To Use Them\n\nCustom metrics allow us to monitor specific aspects of your application beyond the scope of default metrics provided by AWS. The default metrics provided by CloudWatch include instance metrics, traffic mirroring metrics, error metrics, etc. Still, we might need more information about our serverless app.\n\nCustom Metrics come in handy when we need a piece of more specific information like the number of times an API is called, the origin of requests, requests that resulted in a particular error code, metrics with additional information, etc. Custom metrics can provide more context to your metrics by adding¬†custom dimensions¬†that help you filter and group your metrics in CloudWatch.\n\nIt can help in various other aspects like analyzing data over a period of time, creating alarms based on specific thresholds of error requests, number of concurrent users, etc. Custom metrics provide granular control over the aspects of the serverless applications we want to measure. It's up to us to get the insights that help us improve the performance and manage the application.\n\n![Graphic taken from https://blog.awsfundamentals.com](https://cdn.hashnode.com/res/hashnode/image/upload/v1676190070152/488cc3a7-833a-42ee-be1a-872280edb87c.png?auto=compress,format&format=webp&auto=compress,format&format=webp align=\"left\")\n\n## Creating Custom Metrics with SST and CloudWatch\n\nThere are mainly three ways to create custom metrics in CloudWatch.\n\n* API\n    \n* CLI\n    \n* Embedded Metric Format (EMF)\n    \n\nWe are interested in using EMF for the scope of this article.\n\n**Embedded Metric Format (EMF)** is a format that is used to send custom metrics to CloudWatch. CloudWatch can automatically extract custom metrics based on the logs it receives in Embedded Metric Format, allowing us to add alarms or visualise them in detail.\n\nEMF is particularly useful when sending a large number of custom metrics, as it allows you to send them in batches via logs. An embedded metric format looks like this:\n\n```json\n{\n  \"_aws\": {\n    \"Timestamp\": 1574109732004,\n    \"CloudWatchMetrics\": [\n      {\n        \"Namespace\": \"lambda-function-metrics\",\n        \"Dimensions\": [[\"functionVersion\"]],\n        \"Metrics\": [\n          {\n            \"Name\": \"time\",\n            \"Unit\": \"Milliseconds\",\n            \"StorageResolution\": 60\n          }\n        ]\n      }\n    ]\n  },\n  \"functionVersion\": \"$LATEST\",\n  \"time\": 100,\n  \"requestId\": \"989ffbf8-9ace-4817-a57c-e4dd734019ee\"\n}\n```\n\n> [Refer to this documentation for detailed specification on EMF](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Embedded_Metric_Format_Specification.html)\n\n## Sending Custom Metrics to CloudWatch\n\nWe use AWS Lambda Powertool to send custom metrics to CloudWatch. Using the `@aws-lambda-powertools/metrics` library, you can use the `Metrics` class provided by the library. Here is an example of how to send a custom metric to CloudWatch using `@aws-lambda-powertools/metrics`:\n\n```javascript\nimport { Metrics, MetricUnits } from \"@aws-lambda-powertools/metrics\";\n\nconst metrics = new Metrics({\n¬†¬†namespace: \"hashnode\",\n¬†¬†serviceName: \"graphql\",\n});\n\nexport const handler = async () => {\n¬†¬†const singleMetric = metrics.singleMetric();\n  singleMetric.addDimension('origin', 'app');\n  singleMetric.addMetric('count', MetricUnits.Count, 1);\n  metrics.publishStoredMetrics();\n};\n```\n\nIn the example above, we have defined the custom metric with namespace, dimension and unit. Let's understand them a little better:\n\n### 1\\. Namespaces\n\nA namespace is a container for CloudWatch metrics. We can create multiple namespaces for different metrics and distinguish them from each other. For example, AWS Amplify stores the metrics under `AWS/AmplifyHosting` namespace.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1679914036400/2153463e-88b2-48ee-8a3e-58ea47cfea3f.png align=\"center\")\n\nIn the above example, we are creating a custom namespace `hashnode` to group all the Hashnode-related metrics. It will look something like this on the console:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1679914124331/833ae87c-1c62-4e19-9903-e1f79db701ff.png align=\"center\")\n\n### 2\\. Dimensions\n\nA dimension is a name-value pair that help us describe and categorise the metric. Custom dimensions are additional information that helps us filter and group metrics in CloudWatch.  \nIn the above example, we are adding a dimension `origin` to track the origin of a request. This `origin` dimension can be used to group and filter metrics by the origin of the request. Another example of dimension could be adding `stage` property, which keeps track of the environment of the request. It can be `production`, `dev`, or `staging` which can further be used for filtering or sorting.  \nIt will appear like this in the AWS console:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1679980280508/49a0eb8b-cf4d-49e4-87f0-55b0a60d1fea.png align=\"center\")\n\n### 3\\. Units\n\nA unit is simply the data type we use as a measure. A unit can be `Bytes`, `Seconds`, `Microseconds`, `Count` and `Percent`, etc. In our case, we are using `Count` as a unit and setting its value to one.\n\n> [Complete list of supported units can be found here.](https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/API_MetricDatum.html)\n\nThe resulting log will look something like this in the embedded metric format:\n\n```json\n{\n    \"_aws\": {\n        \"Timestamp\": 1679312949002,\n        \"CloudWatchMetrics\": [\n            {\n                \"Namespace\": \"hashnode\",\n                \"Dimensions\": [\n                    [\n                        \"origin\"\n                    ]\n                ],\n                \"Metrics\": [\n                    {\n                        \"Name\": \"count\",\n                        \"Unit\": \"Count\"\n                    }\n                ]\n            }\n        ]\n    },\n    \"service\": \"graphql\",\n    \"order\": 1\n}\n```\n\n## How to Check Custom Metrics on CloudWatch\n\nOnce we are done with adding custom metrics, we can head over to CloudWatch to check the logs. Go to¬†[the metrics overview](https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#metricsV2:graph=~())¬†in CloudWatch and find the custom namespace we just added.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1679312473318/9fb3fab6-6645-4e94-b960-f8c6a0799621.png align=\"center\")\n\nIf you click on the namespaces, you will see your metrics.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1679915022589/35fccffe-b78d-49f4-9509-d6db2f064242.png align=\"center\")\n\n## How We Are Using Custom Metrics at Hashnode\n\nAt Hashnode, we use custom metrics in multiple ways. Let's delve into some of these metrics and explore them thoroughly.\n\n### Tracking Origins of GraphQL Requests\n\nWe use GraphQL APIs at Hashnode, which are utilized by multiple apps, including the community front, the blog front, and the mobile app. Knowing the origin of these requests is essential to allocate resources accordingly. Custom Metrics help us identify the number of requests originating per app.\n\n![Table of API origin, stage and count organised by origin](https://cdn.hashnode.com/res/hashnode/image/upload/v1680765732583/d629e77a-dbf8-4b6c-a412-4400a7546510.png align=\"center\")\n\n### Tracking Application Version and OS for Mobile App\n\nWe use custom metrics to track usage by each application version and OS for Hashnode mobile app. Multiple active mobile app versions can be installed on different devices on different platforms. Custom Metrics have proven invaluable in providing great insight into the mobile app user base.\n\n### Tracking Errors via Error Code\n\nWe use custom metrics to record errors by error codes and rearrange them for easy monitoring. We can get insights about errors thrown per app/lambda categorized by the error code.  \nMetrics associated with error codes allow us to have precise control over what occurs during an incident. For instance, we can associate a 5xx error code with an AWS alarm that can heighten the issue and notify the team during downtime. Refer to the next section for more info on how to use alarms with metrics.\n\n![Table of API and Metrics organised by error code](https://cdn.hashnode.com/res/hashnode/image/upload/v1680765483642/8d9b5cf1-3d5f-4c53-8ccb-7236d053aec7.png align=\"center\")\n\n### Using Data to Set Alarms\n\nWe utilize the historical data of the metric when monitoring errors by error code, and set up alarms to notify us when it surpasses a certain threshold.  \nFor example, we define threshold as `\"Errors >= 1 for 1 datapoints within 1 minute\"` under conditions while creating an alarm. This will ensure that the associated action is triggered when the threshold is reached.  \nThe action can be enabled to send one or more notifications via SNS informing the person on-call.\n\n![Screenshot showing usage of Metrics to set alarms under conditions and enable actions](https://cdn.hashnode.com/res/hashnode/image/upload/v1680765777862/7d348157-8bd3-4df9-8dbf-e9cf10c67959.png align=\"center\")\n\n## Conclusion\n\nAdding custom metrics to your serverless application is essential for monitoring and optimizing its performance. Embedded Metric Format provides an easy way to send logs and extract custom metrics to CloudWatch for monitoring. We also learned how we utilize custom metrics at hashnode.\n\nDo let us know if this article was helpful in the comments.\n\n> [Checkout this article for a more detailed guide on Custom Metrics](https://blog.awsfundamentals.com/optimize-your-application-monitoring-with-cloudwatch-custom-metrics)."},"author":{"name":"Shad Mirza","profilePicture":"https://cdn.hashnode.com/res/hashnode/image/upload/v1663070035311/JaSbIMfve.jpg"},"coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/upload/v1680672874136/f3bfe7ef-8741-4236-b798-012fa2687f0b.png"},"publishedAt":"2023-04-19T06:00:13.019Z","slug":"custom-metrics-made-easy-a-comprehensive-guide-to-sst-and-cloudwatch-integration","brief":"Monitoring and managing resources is an important part of making sure your serverless application is working as expected and troubleshooting when issues arise. Amazon Web Services (AWS) provides a wide range of services to monitor and track the perfo..."},{"id":"63f46b9b4cd7a287eab5c0c7","title":"From Chaos to Clarity: Understanding GraphQL Error Codes with Stellate, AWS Lambda, API Gateway, and Apollo","url":"https://engineering.hashnode.com/understanding-graphql-error-codes-with-stellate-aws-lambda-api-gateway-and-apollo","content":{"markdown":"GraphQL handles error codes a bit differently compared to REST Apis. While we still get HTTP response codes like\n\n* **200** OK\n    \n* **400** Bad Request\n    \n* **500** Server Error\n    \n\nIt often happens that an error happened even if we received a 200 status code.\n\nThis blog post is about **the internals of how Hashnode** uses error codes for debugging and understanding the system. This is also for you my fellow Hashnode colleagues ü§ó\n\nThis post is **not a complete guide** to GraphQL error metrics. Each framework handles it a bit differently and I am not considering myself an expert in each of them.\n\n## Hashnode's Architecture - Stellate, API Gateway, Lambda, Apollo\n\nTo be able to understand how everything interacts I first give you an introduction to our internal Hashnode Architecture.\n\n![Hashnode's API Infrastructure](https://cdn.hashnode.com/res/hashnode/image/upload/v1676451250250/bee89246-4aa7-482b-a731-776d3b96b89f.png align=\"center\")\n\nWe have a serverless-first mindset. That means everywhere where it makes sense we use serverless services. We don't want to build & manage infrastructure ourselves.\n\nOur API consists of the following services:\n\n* **API Gateway** with AWS Lambda as the main API\n    \n* **Apollo Server** as the GraphQL server\n    \n* [**Stellate**](https://stellate.co) **CDN** as an Edge Cache, Analytics, and Error Software\n    \n\nTo cache our responses on the edge we use [Stellate](http://stellate.co). Stellate gives your GraphQL API superpowers! Automatic alerts, analytics, rate limits, and much more.\n\nIn between sits an **HTTP API Gateway**. API Gateway integrates with Lambda. It forwards each request and sends it to the Lambda function. Lambda creates the Apollo Handler on each invocation (for new containers) and fulfills the request.\n\nAPI Gateway and Lambda have both **status codes**. This is where it gets interesting.\n\n* **API Gateway Status Code:** This is the status code of the receiver of this API (i.e. Stellate) that receives\n    \n* **Lambda Integration Status Code**: This is the status code API Gateway receives\n    \n\nThis is not necessarily the same status code! There is even a third response status code, which is the response code **by Stellate**. Most of the time (almost always) this mirrors the status code of the API Gateway.\n\nBy using Stellate we were now also aware of all the different error codes our API consumers received.\n\n![Different Status Codes by Stellate](https://cdn.hashnode.com/res/hashnode/image/upload/v1675431258965/7d951c8f-0c81-4819-9cc4-f797e5e6bf70.png align=\"center\")\n\nThat's the reason for this blog post. Let's dive a bit deeper into the errors GraphQL returns to you.\n\n## An Overview of HTTP Status Codes\n\nGraphQL is still a typical REST call. You make a POST call to an endpoint and you will receive an HTTP response. This response can have several status codes.\n\nLet's first look over the status codes **briefly**, then focus on some detailed examples and see how Stellate, API Gateway, and Lambda react.\n\n### 200 - OK, or Is It? ü§î\n\n200 in HTTP means the request was **successful**. This is one of the **confusing codes** in GraphQL.\n\n200 doesn't tell that no error happened. It only means that the request was successful. You still need to check your actual response to see if an error was returned. If an error in your GraphQL API happened it will be visible by having an `GRAPHQL_ERROR_CODE` in the response body.\n\n### 400 - Bad Request\n\nGraphQL APIs are based on a schema. Our schema for a publication for example is the following:\n\n```graphql\n  type Publication implements Node {\n    id: ID!\n    title: String!\n    ...\n}\n```\n\nIf you want to access a field that is not implemented in this schema you get an error code 400 - `GRAPHQL_VALIDATION_FAILED`. 400 in general are validation errors.\n\n### 500 - Server Error\n\n500 still means server errors. 500 are still **bad errors**. That means you need to check them. They should not happen. But be aware. Understanding where the error actually appeared can be quite a challenge in the beginning.\n\n500 is a **server error** but what is our server in the architecture? Is it the lambda handler? The Apollo Handler, or anything in between?\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1675433481488/6d7b62a8-c1f5-4a5c-bf75-e2c0e25dd263.png align=\"center\")\n\nThe Lambda function has two components:\n\n1. **Lambda Function**: This is everything that happens **before** the Apollo server starts, e.g. connecting to the DB, fetching secrets, etc.\n    \n2. **Apollo Handler**: This is the Apollo server handling requests.\n    \n\n500 **in our architecture** means there is something wrong with the **Lambda function** itself. This doesn't mean something is wrong with the **Apollo Handler** but with the **Lambda Function**. We will see both examples in the example section.\n\n## Real Scenarios - Let's see some Examples\n\nOkay, so far the theory. I hope you are still with me. Let's now dig a bit deeper and understand some example scenarios.\n\n### Successful Request - The Happy Path ü§ó\n\nA user sends a correct query, for example, this one:\n\n```graphql\nquery {\n    publication(host:\"jannikwempe.hashnode.net\") {\n        id\n        author {\n            name\n        }\n    }\n}\n```\n\nStellate receives this query, and forwards it to API Gateway, API Gateway creates the event and invokes the Lambda function. Lambda queries the database and sends the response back to API Gateway. This is how everything comes together.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1675433832056/18008a34-cca8-4a08-9d4b-31d12cd72c4d.png align=\"center\")\n\nAll states are 200 and everything is fine.\n\nLet's see the example in Postman:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1676452245755/1f25e958-80f4-43b8-b51a-5f8245e31802.png align=\"center\")\n\nWe receive the response we expect with the status code we expect üëçüèΩ\n\n### Error in Apollo Handler - 200 NOT OK ‚ùå\n\nNow let's see an example of the mysterious 200 response with an error.\n\nWe mock an error by throwing an error from **the Apollo Handler**. Remember, Apollo Handler != Lambda Handler necessarily.\n\nIn the Lambda Handler function we create the Apollo Handler like that:\n\n```typescript\nconst serverHandler: Handler<any, any> = (...args) => {\n...\n  return server.createHandler()\n...\n};\n```\n\nEverything that happens within the `server` will be an error **within the handler.** To mock this behavior I've added a `throw new Error()` somewhere in querying a publication.\n\nThe result looks like that:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1675433894562/98f1d799-6b8d-4a85-870a-c4b414d6d85f.png align=\"center\")\n\nIf we are now calling the same query we see the following result:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1676452532352/e31f41da-641a-40ba-afe7-bdaa4e94cb18.png align=\"center\")\n\nFor us, this was new. Even for an **undefined** error and a clear **server error,** the API responds with a 200 response code. The actual error code is in the response and maps on the error code `INTERNAL_SERVER_ERROR`. You can define this behavior of course.\n\nThat is where Stellate is doing a great Job. Without the need of creating any extra logic, Stellate shows us the error in their Error Dashboard:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1676452754438/87919f94-7b20-478f-b84b-c23eb6102a78.png align=\"center\")\n\n**Learning from this scenario:** You need to have alerts on 200 response codes like `INTERNAL_SERVER_ERRORS` as well.\n\n### Server Error, for Real -&gt; 500\n\nThe next scenario we look at is a proper server error. This time not the Apollo Handler but the **Lambda Function itself** throws an undefined error.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1676453590617/d9d8f0eb-6e51-4eae-b2ed-600717bcf222.png align=\"center\")\n\nThe Lambda function is defined as everything that happens **before** the Apollo server is created. In our scenario, this is mainly connecting to our database and caching the connection in the Lambda context. We are doing this by using the amazing [middy middleware library](https://middy.js.org/).\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1676453319426/a0176449-4ad3-49d4-bf5c-1b6ef4c78dbf.png align=\"center\")\n\nI introduced an error by throwing an undefined error in the middleware that connects to the DB.\n\nLet's see what it looks like in Postman:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1676453490262/0f6b1e64-5758-4baf-94da-0f153f712c29.png align=\"center\")\n\nAh, this time we get a proper 500 error code! Which makes sense of course. No GraphQL server started so there is no way to parse errors in a different way than a normal REST API.\n\nIn Stellate it looks like that:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1676453669536/f3c5d744-ed8f-4499-9bc8-ba2e22f5471e.png align=\"center\")\n\n**Learning from this scenario:** A 500 error means there is something wrong with the underlying \"infrastructure\" which is your Lambda function. This makes debugging a whole lot easier.\n\n### GET Request on Stellate vs. directly on API Gateway\n\nGraphQL only uses the HTTP Method **POST** for serving data. As a company with our scale, we see many people trying out our internal APIs as well and trying to come through üïµüèΩ\n\nSince we'd like to understand how our system behaves in different scenarios we also looked at that one.\n\nIf you send a GET request to the Stellate endpoint you will get the following response: 204 - No Content\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1676454196106/01a6de93-a381-48fe-b2fc-688b4ed62f99.png align=\"center\")\n\nInterestingly enough, if you send the same request **directly to the API Gateway** you will get a 400 response code.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1676454225263/42cdda69-97de-4adc-be3d-b587b3c9501d.png align=\"center\")\n\nThe main important thing. Requests like that shouldn't even be able to enter your API.\n\n**Learning from this scenario:** Understand that some response codes behave differently on Stellate and on API GW.\n\n### Validation Errors\n\nNow we will look at some validation errors, i.e. 400 error codes. A validation error means that there is something wrong with your query or mutation. While you get 400 as a response code you will also get a GraphQL error code in your response.\n\n#### Access Invalid Field\n\nLet's start by trying to access a field that doesn't exist.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1676454571665/62ddeb0d-3327-40b1-b4b4-dec4b559f510.png align=\"center\")\n\nI try to send the following query to our API:\n\n```graphql\n{\n   publication(host:\"jannikwempe.hashnode.net\") {\n       id\n       bla\n   }\n}\n```\n\nThis will result in a response code 400 -&gt; validation failing.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1676465439853/f42eea63-4e8f-4197-b92b-ed06be943c8c.png align=\"center\")\n\nWe receive the error `GRAPHQL_VALIDATION_FAILED`. So far so good.\n\n#### Type Check Fails\n\nSomething similar happens if you provide the wrong type. Instead of passing the `username` as a string, I've added the `username` as a number here.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1676465586589/88cc6908-bf70-4d28-9179-04978ba314d3.png align=\"center\")\n\nThis will also result in the following response\n\n```json\n{\n    \"errors\": [\n        {\n            \"message\": \"String cannot represent a non string value: 123\",\n            \"extensions\": {\n                \"code\": \"GRAPHQL_VALIDATION_FAILED\",\n            }\n        }\n    ]\n}\n```\n\n#### 400 but a server error\n\nOne error that took some while for us to understand was the following one:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1676465758320/d59330d4-2a2f-456e-ac2a-d3a7974f7856.png align=\"center\")\n\nIn Stellate we received lots of error messages with the response code 400. But the actual GraphQL error code was `INTERNAL_SERVER_ERROR`.\n\nSo what now? Validation error because of the response code 400? Or an internal server error? But why isn't that a 200 error then like in the first examples?\n\nBy taking a closer look we saw that none of these exceptions had a query attached. By trying to reproduce it we saw that some people are trying out to send empty queries to our API. This results in a server error with the response code 400.\n\nCases like these can be adjusted manually in Apollo. But by default, it behaves like that.\n\n**Learning from this scenario:** Understand that the GraphQL error code is the most important piece to understand. In this case, `INTERNAL_SERVER_ERROR` is an expected code and nothing is wrong with your server.\n\n### Playground deactivated - 403 vs. 400\n\nOne last error we faced a lot at times is a 403 error from Stellate:\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1676466176458/4281a5e9-abbe-42da-98a7-4cae155225c7.png align=\"center\")\n\nThis error indicates that somebody is doing a `GET` request on our API. But when I tried to reproduce that I got a `204 - No Content` like seen above. It took some time to understand that this error message results if somebody tries to access a disabled GraphQL playground. When I try to access the **Stellate Playground** I get a 403 Forbidden error.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1676466433979/2249a5b1-5b88-43dd-8888-ffb16e4a4ffb.png align=\"center\")\n\n**But** if I try to access the GraphQL Playground directly in Apollo and it is deactivated I get a 400 error.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1676466476500/83e0ad24-9a4a-4dbe-b440-9f337fa79c8c.png align=\"center\")\n\nThis is nothing major. It is still important for us to understand the differences. With that, we are able to act on real 403 issues.\n\n## Summary\n\nIn summary, it is really important to understand the ins and outs of your API. Stellate makes our lives much easier by automatically parsing GraphQL error codes and showing the resulting response codes. In the end, everything is dependent on the implementation of your GraphQL server.\n\nTo be able to act quickly on incidents or abnormal behavior it is critical to understand what each error code and error message actually means. Many errors are expected. Especially in cases like:\n\n* Unauthenticated access\n    \n* Validation Errors\n    \n\nBut there are also many cases where they are not expected.\n\nWe have created several alarms and in special cases also an automated generation of Incidents (üëãüèΩ Better Uptime) so that we are able to act quickly on all upcoming incidents.\n\nThanks for sticking with me, see you soon üëãüèΩ"},"author":{"name":"Sandro Volpicella","profilePicture":"https://cdn.hashnode.com/res/hashnode/image/upload/v1627027343841/bT6HIivZm.jpeg"},"coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/upload/v1676964092528/216d132a-88bd-44b8-a263-9001cb3fdc4b.png"},"publishedAt":"2023-02-21T06:58:35.770Z","slug":"understanding-graphql-error-codes-with-stellate-aws-lambda-api-gateway-and-apollo","brief":"GraphQL handles error codes a bit differently compared to REST Apis. While we still get HTTP response codes like\n\n200 OK\n\n400 Bad Request\n\n500 Server Error\n\n\nIt often happens that an error happened even if we received a 200 status code.\nThis blog pos..."}]},"__N_SSG":true}