{"pageProps":{"post":{"slug":"resolving-high-disk-space-utilization-in-mongodb","url":"https://engineering.hashnode.com/resolving-high-disk-space-utilization-in-mongodb","brief":"Problem\nWe have recently received many alerts about high disk usage on MongoDB Atlas for the past two weeks.\nAccording to docs, Disk utilization % on Data Partition occurs if the percentage of time during which requests are being issued to any partit...","title":"Resolving High Disk Space Utilization in MongoDB","publishedAt":"2023-06-22T06:30:39.500Z","coverImage":{"url":"https://cdn.hashnode.com/res/hashnode/image/upload/v1686811735224/86d7f207-efe4-481c-a5d1-766a621cf262.png"},"author":{"name":"Vamsi Rao","profilePicture":"https://cdn.hashnode.com/res/hashnode/image/upload/v1665498938042/VRcVGyEcb.jpeg"},"id":"6493ea8ff012983651c312d1","content":{"markdown":"### Problem\n\nWe have recently received many alerts about high disk usage on MongoDB Atlas for the past two weeks.\n\nAccording to docs, [Disk utilization % on Data Partition](https://www.mongodb.com/docs/atlas/reference/alert-resolutions/disk-io-utilization/) occurs if the percentage of time during which requests are being issued to any partition that contains the MongoDB collection data meets or exceeds the threshold. The downside of the Disk Utilization being high is the DB cannot process other queries if it maxes out. This can lead to data loss or inconsistencies.\n\nThese are how the metrics looked like in one of the last alerts. From the graph, Disk Util % at 4:30UTC was around 99.86%.\n\n![Max disk util % metric hitting maxing out at 100%](https://cdn.hashnode.com/res/hashnode/image/upload/v1687156481558/9c36fb63-e013-43cb-81da-082359d0038a.png align=\"center\")\n\nChecking the profiler, we can notice a query that's running during the same time, and it takes around `17s` to resolve!\n\n![a screenshot of a computer screen with a number of numbers on it](https://cdn.hashnode.com/res/hashnode/image/upload/v1682316589744/77e990f3-5a5e-4285-9e72-def7d2465829.png align=\"center\")\n\nThis query is related to a `KinesisAnalyticsLogs` collection. The collection internally is used to record views of different posts. The mentioned query already makes use of an index and still takes that much time to resolve because of the sheer size of the collection.\n\n![a screen shot of a web page with a number of items on it](https://cdn.hashnode.com/res/hashnode/image/upload/v1682312447723/bcf12123-229c-4247-bdf3-7bab64c757fa.png align=\"center\")\n\nThe total number of documents is close to 70 million, and the index size itself is close to a GB! That seems to be why it takes so long to resolve. Since this collection was recording analytical data, it was bound to reach this volume at some point. Along with that, from the profiler image, we can see that the query has yielded ~1300 times. According to [docs](https://www.mongodb.com/docs/manual/reference/database-profiler/#mongodb-data-system.profile.numYield), if a query yields a lot, then it is hitting a disk a lot. If we want that query to be faster, then the data needs to be in memory (index).\n\nUpon digging further, this query is scheduled to run every 30mins to sync the views. So we can correlate high query times on the profiler and Disk IOPS peaking almost simultaneously.\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1682317743687/01f4c579-2d4a-4fec-bec3-9c3fec72bc76.png align=\"center\")\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1682317793584/30aa75ff-0132-4b1f-9547-e217f158b5f1.png align=\"center\")\n\n### Solutions\n\nBased on the investigation, we came up with two solutions:\n\n**Short Term Solution**\n\n* Since the collection size is a problem, and we are not using older data, deleting records older than a month will reduce the collection size drastically, leading to a smaller index size and faster query resolution.\n    \n* We can also add `TTL` to the records in `kinesisAnalyticsLogs` collection ([https://hashnode.com/rix/share/sg4lYYf\\_M](https://hashnode.com/rix/share/sg4lYYf_M)). It'll automatically delete the records older than a month going forward. This will make the index smaller and lead to a shorter query time.\n    \n\n**Long Term Solution**\n\nData like views/analytics should not be stored in the same place as the core business data. This collection will keep growing by the minute since it records the views. Some other DB should be used that's more appropriate for it.\n\n### Implementation\n\nWe decided to go with the short-term solution for now and added the long-term solution as a future task. For starters, we added TTL indexes immediately. With this, all the future records that will be created will be automatically deleted after the expiry time. This index can only be set on a field type `date`\n\n```javascript\nkinesisAnalyticsLogSchema.index({ dateAdded: 1 }, { expireAfterSeconds: 2592000 });\n```\n\nTo delete the past records, we ran a script that can delete all the records older than a month. Since we were deleting huge amounts of data within a short span, we encountered some problems while running the script. We had to keep a close eye on some of the metrics so that it didn't lead to a DB restart.\n\n* `CPU% spikes` A large number of record deletions were leading to CPU% usage over 95. We had to be careful and gave enough breathers in between to the DB.\n    \n* `Replication Oplog Window has gone below 1 hour` This was a new alert that we came across. Since our instance had one primary and two secondary DBs (replicas), the secondary DBs require enough time to replicate the writes from the primary. We had to be careful not to go below the recommended 1-hour window.\n    \n\nAfter carefully running the script, this is how the overall collection looked like\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1686812657442/60eba67a-1754-4e8f-8c9f-d6ff3f2bc919.png align=\"center\")\n\nThis was almost 2-3 days of effort to run the script and observe how the DB was performing. We finally were seeing the difference. The query resolution was fast enough for a background aggregate task, and it was not creating the disk util alerts ðŸŽ‰\n\n![](https://cdn.hashnode.com/res/hashnode/image/upload/v1686817342070/33c9d255-4785-40b5-bdf1-5b5b9c4a02e9.png align=\"center\")\n\nOverall improvements:\n\n* Average query time went down to **1.5s** from **15s**\n    \n* The index size went down to **~400MB** from **~1GB**\n    \n* The collection size went down to **~9GB** from **~40GB**","html":"<h3 id=\"heading-problem\">Problem</h3>\n<p>We have recently received many alerts about high disk usage on MongoDB Atlas for the past two weeks.</p>\n<p>According to docs, <a target=\"_blank\" href=\"https://www.mongodb.com/docs/atlas/reference/alert-resolutions/disk-io-utilization/\">Disk utilization % on Data Partition</a> occurs if the percentage of time during which requests are being issued to any partition that contains the MongoDB collection data meets or exceeds the threshold. The downside of the Disk Utilization being high is the DB cannot process other queries if it maxes out. This can lead to data loss or inconsistencies.</p>\n<p>These are how the metrics looked like in one of the last alerts. From the graph, Disk Util % at 4:30UTC was around 99.86%.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1687156481558/9c36fb63-e013-43cb-81da-082359d0038a.png\" alt=\"Max disk util % metric hitting maxing out at 100%\" class=\"image--center mx-auto\" /></p>\n<p>Checking the profiler, we can notice a query that's running during the same time, and it takes around <code>17s</code> to resolve!</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1682316589744/77e990f3-5a5e-4285-9e72-def7d2465829.png\" alt=\"a screenshot of a computer screen with a number of numbers on it\" class=\"image--center mx-auto\" /></p>\n<p>This query is related to a <code>KinesisAnalyticsLogs</code> collection. The collection internally is used to record views of different posts. The mentioned query already makes use of an index and still takes that much time to resolve because of the sheer size of the collection.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1682312447723/bcf12123-229c-4247-bdf3-7bab64c757fa.png\" alt=\"a screen shot of a web page with a number of items on it\" class=\"image--center mx-auto\" /></p>\n<p>The total number of documents is close to 70 million, and the index size itself is close to a GB! That seems to be why it takes so long to resolve. Since this collection was recording analytical data, it was bound to reach this volume at some point. Along with that, from the profiler image, we can see that the query has yielded ~1300 times. According to <a target=\"_blank\" href=\"https://www.mongodb.com/docs/manual/reference/database-profiler/#mongodb-data-system.profile.numYield\">docs</a>, if a query yields a lot, then it is hitting a disk a lot. If we want that query to be faster, then the data needs to be in memory (index).</p>\n<p>Upon digging further, this query is scheduled to run every 30mins to sync the views. So we can correlate high query times on the profiler and Disk IOPS peaking almost simultaneously.</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1682317743687/01f4c579-2d4a-4fec-bec3-9c3fec72bc76.png\" alt class=\"image--center mx-auto\" /></p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1682317793584/30aa75ff-0132-4b1f-9547-e217f158b5f1.png\" alt class=\"image--center mx-auto\" /></p>\n<h3 id=\"heading-solutions\">Solutions</h3>\n<p>Based on the investigation, we came up with two solutions:</p>\n<p><strong>Short Term Solution</strong></p>\n<ul>\n<li><p>Since the collection size is a problem, and we are not using older data, deleting records older than a month will reduce the collection size drastically, leading to a smaller index size and faster query resolution.</p>\n</li>\n<li><p>We can also add <code>TTL</code> to the records in <code>kinesisAnalyticsLogs</code> collection (<a target=\"_blank\" href=\"https://hashnode.com/rix/share/sg4lYYf_M\">https://hashnode.com/rix/share/sg4lYYf_M</a>). It'll automatically delete the records older than a month going forward. This will make the index smaller and lead to a shorter query time.</p>\n</li>\n</ul>\n<p><strong>Long Term Solution</strong></p>\n<p>Data like views/analytics should not be stored in the same place as the core business data. This collection will keep growing by the minute since it records the views. Some other DB should be used that's more appropriate for it.</p>\n<h3 id=\"heading-implementation\">Implementation</h3>\n<p>We decided to go with the short-term solution for now and added the long-term solution as a future task. For starters, we added TTL indexes immediately. With this, all the future records that will be created will be automatically deleted after the expiry time. This index can only be set on a field type <code>date</code></p>\n<pre><code class=\"lang-javascript\">kinesisAnalyticsLogSchema.index({ <span class=\"hljs-attr\">dateAdded</span>: <span class=\"hljs-number\">1</span> }, { <span class=\"hljs-attr\">expireAfterSeconds</span>: <span class=\"hljs-number\">2592000</span> });\n</code></pre>\n<p>To delete the past records, we ran a script that can delete all the records older than a month. Since we were deleting huge amounts of data within a short span, we encountered some problems while running the script. We had to keep a close eye on some of the metrics so that it didn't lead to a DB restart.</p>\n<ul>\n<li><p><code>CPU% spikes</code> A large number of record deletions were leading to CPU% usage over 95. We had to be careful and gave enough breathers in between to the DB.</p>\n</li>\n<li><p><code>Replication Oplog Window has gone below 1 hour</code> This was a new alert that we came across. Since our instance had one primary and two secondary DBs (replicas), the secondary DBs require enough time to replicate the writes from the primary. We had to be careful not to go below the recommended 1-hour window.</p>\n</li>\n</ul>\n<p>After carefully running the script, this is how the overall collection looked like</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1686812657442/60eba67a-1754-4e8f-8c9f-d6ff3f2bc919.png\" alt class=\"image--center mx-auto\" /></p>\n<p>This was almost 2-3 days of effort to run the script and observe how the DB was performing. We finally were seeing the difference. The query resolution was fast enough for a background aggregate task, and it was not creating the disk util alerts ðŸŽ‰</p>\n<p><img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1686817342070/33c9d255-4785-40b5-bdf1-5b5b9c4a02e9.png\" alt class=\"image--center mx-auto\" /></p>\n<p>Overall improvements:</p>\n<ul>\n<li><p>Average query time went down to <strong>1.5s</strong> from <strong>15s</strong></p>\n</li>\n<li><p>The index size went down to <strong>~400MB</strong> from <strong>~1GB</strong></p>\n</li>\n<li><p>The collection size went down to <strong>~9GB</strong> from <strong>~40GB</strong></p>\n</li>\n</ul>\n"},"ogMetaData":{"image":"https://cdn.hashnode.com/res/hashnode/image/upload/v1686811773227/c8f20c9f-790e-4501-b002-7b17a90e14fa.png"}}},"__N_SSG":true}